{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Run NLP Experiments using the Feedly API.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/secsilm/awsome-colabs/blob/master/Run_NLP_Experiments_using_the_Feedly_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "FLbhLc-IzW3N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Run NLP Experiments using the Feedly API\n",
        "\n",
        "## Introduction\n",
        "\n",
        "We live in a world where the volume of information is increasing exponentially. We believe that advances in Machine Learning and Natural Language Processing can help process and prioritize information more efficiently.\n",
        "\n",
        "The goal of this notebook is to help you create a simple KNN classifier that takes as input a Feedly board with 50 articles about AI and learns to predict which articles from The Verge and Engadget are about AI. All this in less than 20 minutes.\n",
        "\n",
        "You will learn:\n",
        "1. How to connect to a Feedly account using the Feedly API and download articles from feeds and boards\n",
        "2. How to train a classifier to reliably classify AI and non-AI articles\n",
        "3. How to apply the classifier to new articles\n",
        "4. How to save articles to a Feedly board\n",
        "\n",
        "[See blog post for more information](https://blog.feedly.com)"
      ]
    },
    {
      "metadata": {
        "id": "BaFI_TDEeUTB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Terminology Mapping: There is sometimes a gap between the terms used in the Feedly UI and the concepts used in the Feedly API. What is called a board in the UI is labeled tags in the API. What is called feed in the UI is labeled category in the API.\n"
      ]
    },
    {
      "metadata": {
        "id": "wmCqZUiJ0yrO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "If you do not have a Feedly client, you can create one for free on https://feedly.com \n",
        "\n",
        "Let's install the feedly client and the NLP library gensim first."
      ]
    },
    {
      "metadata": {
        "id": "1_KvshIIfsEq",
        "colab_type": "code",
        "outputId": "d8af552e-215f-4adc-826d-f253cb875509",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install feedly-client gensim --quiet"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mspacy 2.0.16 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mgoogle-colab 0.0.1a1 has requirement requests~=2.18.0, but you'll have requests 2.20.1 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eTjxs6Psge_V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from feedly.session import FeedlySession\n",
        "from feedly.data import StreamOptions, Streamable\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import urllib3\n",
        "urllib3.disable_warnings()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "73mn9eP21Wsm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Connect to your Feedly\n",
        "\n",
        "To log in to your account, you should get your feedly token. You can find it in the `console` page of your account [here](https://feedly.com/i/console) (recommended) by clicking on \"Copy Token to clipboard\"\n",
        ". \n",
        "Or if you want to do more advanced stuff you can create a  [developer token](https://developer.feedly.com/v3/developer/) instead. \n",
        "\n",
        "To download articles, you first have to pick one of your feeds with `category = sess.get_category(category_uuid)` and then do `category.stream_contents()`. Let's see how it works."
      ]
    },
    {
      "metadata": {
        "id": "JqcyXIYEgIlL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO: Enter your feedlyToken here\n",
        "token = None\n",
        "\n",
        "if token is None:\n",
        "  raise ValueError(\"Please enter a token\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YRSG5yYuga2n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# initialize session\n",
        "sess = FeedlySession(auth=token)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "54wfvo-xgnuE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# print the names of all your feeds (categories) and boards (tags)\n",
        "print(\"Personal feeds:\")\n",
        "for category_uuid, category_data in sess.user.get_categories().items():\n",
        "  print(\"\\t\", f'\"{category_uuid}\"', f\"({category_data['label']})\")\n",
        "print(\"Personal boards:\")\n",
        "for tag_name in sess.user.get_tags():\n",
        "  print(\"\\t\",f'\"{tag_name}\"')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WJT9O8QfgqP9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# print the titles of some articles\n",
        "\n",
        "# get your first category\n",
        "personal_category_uuid = list(sess.user.get_categories().keys())[0]\n",
        "\n",
        "# download articles\n",
        "entries = sess.user.get_category(personal_category_uuid).stream_contents()\n",
        "entries = list(entries)\n",
        "\n",
        "# get the titles in the json property\n",
        "print(f\"Articles from your personal feed '{personal_category_uuid}':\")\n",
        "for entry in entries[:5]:\n",
        "\tprint(\"\\t\"+entry.json[\"title\"])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y72ta6ib2hVp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now that you know how to get articles from your feedly, let's play with them to build a classifier !"
      ]
    },
    {
      "metadata": {
        "id": "6fC6doJ224MX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train your Machine Learning  algorithm\n",
        "\n",
        "Feedly makes it easy to aggregate articles from multiple sources in one place. The Feedly API allows you to get a normalized JSON representation of all the articles aggregated in your Feedly.\n",
        "\n",
        "Letâ€™s try to create a KNN classifier that determines which articles from your Tech feed (The Verge and Engadget) are related to the AI topic.\n",
        "\n",
        "If you do not have a Tech feed, you can use the Add Content to quickly create a new Tech feed and add The Verge and Engadget to that feed.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "8hOFksca5exu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To build a classifier, we need to train a model with positive and negative examples, so that it will be able te recognize new positive articles. Therefore we need a positive board with examples of 'on-topic' articles. You can spend 10-15min on your feedly to save interesting articles into a positive board (>20 articles, the more the better), or you can use a board we already built for you that contains articles about Artificial Intelligence.\n",
        "\n",
        "We will call this board your `positive_board`"
      ]
    },
    {
      "metadata": {
        "id": "pEzIqmmA-sZ_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![machine learning filter](https://blog.feedly.com/wp-content/uploads/2019/01/Screenshot-2019-01-24-16.22.27-1024x548.png)"
      ]
    },
    {
      "metadata": {
        "id": "rJinKyky42Dt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1 Gather the data\n",
        "Let's see how you can build a model that can find 'on-topic' articles from the noise of a source feed.\n",
        "We suppose that we can use the noise of your source feed as a negative class for a classifier (no need to have a negative board !)."
      ]
    },
    {
      "metadata": {
        "id": "aso8s1LRHVm5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# print all your personal feeds\n",
        "print(\"Here are all your personal feeds:\")\n",
        "for category_uuid, category_data in sess.user.get_categories().items():\n",
        "  print(\"\\t\", f'\"{category_uuid}\"', f\"({category_data['label']})\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZI7Y3wdOHtGR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO Leave 'None' to use the default source feed with content from The Verge and Engadget, or select your category (uuid) here.\n",
        "source_feed = None\n",
        "\n",
        "if source_feed is not None and source_feed not in sess.user.get_categories():\n",
        "  raise ValueError('Please select an existing feed')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PbY2J5EGKjeh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Here are all your boards:\")\n",
        "for tag_name in sess.user.get_tags():\n",
        "  print(\"\\t\",f'\"{tag_name}\"')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PQOq7cqkKgit",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO: Leave 'None' to use the default board with articles about AI, or enter your positive articles board.\n",
        "positive_board = None\n",
        "\n",
        "if positive_board not in sess.user.get_tags() and positive_board is not None:\n",
        "  raise ValueError('Please select an existing board')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y70V8DNX29Nc",
        "colab_type": "code",
        "outputId": "2ba80f59-2e73-4458-806d-0b829baccefd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# download the articles\n",
        "\n",
        "# positive articles from a board (if None, download articles about AI)\n",
        "if positive_board is not None:\n",
        "  positive_entries = list(sess.user.get_tag(positive_board).stream_contents())\n",
        "else:\n",
        "  print(\"downloading articles about AI...\")\n",
        "  import urllib.request, pickle, zlib\n",
        "  url = \"https://s3.feedly.com/assets/50_ai_articles.pickle.gz\"\n",
        "  response = urllib.request.urlopen(url)\n",
        "  positive_entries = pickle.loads(zlib.decompress(response.read(), 16+zlib.MAX_WBITS))\n",
        "n_positive = len(positive_entries)\n",
        "\n",
        "# negative articles from the source feed\n",
        "if source_feed is not None:\n",
        "  negative_entries = list(sess.user.get_category(source_feed).stream_contents(options=StreamOptions(max_count=4*n_positive)))\n",
        "else:\n",
        "  print(\"downloading content from The Verge and Engadget...\")\n",
        "  default_sources_ids = \"feed/http://www.theverge.com/rss/full.xml,feed/http://www.engadget.com/rss-full.xml\"\n",
        "  negative_entries = list(Streamable({'id': default_sources_ids}, sess).stream_contents(options=StreamOptions(max_count=4*n_positive)))\n",
        "n_negative = len(negative_entries)\n",
        "\n",
        "# print results\n",
        "if n_positive<15: raise Exception(f\"You should have at least 15 positive articles (you only have {n_positive} articles at the moment)\")\n",
        "print(f\"loaded {n_positive} positive articles and {n_negative} negative articles\")\n",
        "entries = positive_entries + negative_entries\n",
        "labels = np.concatenate((np.ones(len(positive_entries)), np.zeros(len(negative_entries))), axis=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading articles about AI...\n",
            "downloading content from The Verge and Engadget...\n",
            "loaded 50 positive articles and 200 negative articles\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "C3J8ZNokkeA6",
        "colab_type": "code",
        "outputId": "dd15869c-32b2-416c-96e1-ba40278b77d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "# build the dataframe\n",
        "\n",
        "def get_text(entry):\n",
        "  full_content = entry.json[\"fullContent\"] if \"fullContent\" in entry.json else \"\"\n",
        "  content = entry.json[\"content\"][\"content\"] if \"content\" in entry.json else \"\"\n",
        "  summary = entry.json[\"summary\"][\"content\"] if \"summary\" in entry.json else \"\"\n",
        "  title = entry.json[\"title\"]\n",
        "  best=max(full_content, content, summary, title, key=len)\n",
        "  return BeautifulSoup(best.replace(\"\\n\", \"\"), 'html.parser').text\n",
        "\n",
        "def build_dataframe(entries):\n",
        "  df = pd.DataFrame()\n",
        "  df[\"eid\"] = [e.json.get(\"id\") for e in entries]  # id of the entry\n",
        "  df[\"title\"] = [e.json[\"title\"] for e in entries] # title of the entry\n",
        "  df[\"content\"] = [get_text(e) for e in entries]   # text content of the entry \n",
        "  return df\n",
        "\n",
        "df = build_dataframe(entries)\n",
        "df[\"label\"] = labels\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eid</th>\n",
              "      <th>title</th>\n",
              "      <th>content</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PSNTZO8gXFUe+cpCZyApw0vEKWPT4b14D6teBEocIAE=_1...</td>\n",
              "      <td>Facebook sets a new task for AI: guide a virtu...</td>\n",
              "      <td>How do you teach computers to understand langu...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>PSNTZO8gXFUe+cpCZyApw0vEKWPT4b14D6teBEocIAE=_1...</td>\n",
              "      <td>Appleâ€™s new AI chief might actually be the rig...</td>\n",
              "      <td>What is John Giannandrea, Googleâ€™s former hea...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>t9NJb6rMt3WVivgF5JJtqOwhhWhdxPuJjlu7LCw1vdk=_1...</td>\n",
              "      <td>Banks are already bumping up against the limit...</td>\n",
              "      <td>While big tech companies might not face regu...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>t9NJb6rMt3WVivgF5JJtqOwhhWhdxPuJjlu7LCw1vdk=_1...</td>\n",
              "      <td>Itâ€™s Googleâ€™s turn to ask the questions</td>\n",
              "      <td>At Googleâ€™s annual developer conference this ...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PSNTZO8gXFUe+cpCZyApw0vEKWPT4b14D6teBEocIAE=_1...</td>\n",
              "      <td>The White House has set up a task force to hel...</td>\n",
              "      <td>The White House has set up a new task force d...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 eid  \\\n",
              "0  PSNTZO8gXFUe+cpCZyApw0vEKWPT4b14D6teBEocIAE=_1...   \n",
              "1  PSNTZO8gXFUe+cpCZyApw0vEKWPT4b14D6teBEocIAE=_1...   \n",
              "2  t9NJb6rMt3WVivgF5JJtqOwhhWhdxPuJjlu7LCw1vdk=_1...   \n",
              "3  t9NJb6rMt3WVivgF5JJtqOwhhWhdxPuJjlu7LCw1vdk=_1...   \n",
              "4  PSNTZO8gXFUe+cpCZyApw0vEKWPT4b14D6teBEocIAE=_1...   \n",
              "\n",
              "                                               title  \\\n",
              "0  Facebook sets a new task for AI: guide a virtu...   \n",
              "1  Appleâ€™s new AI chief might actually be the rig...   \n",
              "2  Banks are already bumping up against the limit...   \n",
              "3            Itâ€™s Googleâ€™s turn to ask the questions   \n",
              "4  The White House has set up a task force to hel...   \n",
              "\n",
              "                                             content  label  \n",
              "0  How do you teach computers to understand langu...    1.0  \n",
              "1   What is John Giannandrea, Googleâ€™s former hea...    1.0  \n",
              "2    While big tech companies might not face regu...    1.0  \n",
              "3   At Googleâ€™s annual developer conference this ...    1.0  \n",
              "4   The White House has set up a new task force d...    1.0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "vkoOKLwLEyCw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now that we have our dataset, let's preprocess the text content and build a model !"
      ]
    },
    {
      "metadata": {
        "id": "G2JkRMDgE5Z2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Preprocess the articles content with TF-IDF\n",
        "\n",
        "We first do lemmatization and stemming before building a TF-IDF vector for each article.\n",
        "\n",
        "Lemmatization and stemming only keep the stem of words.\n",
        "\n",
        "Example:\n",
        "```\n",
        "technology, technologies -> technolog\n",
        "learnt, learns, learning -> learn\n",
        "```\n",
        "We also remove stop words and we tokenize the texts (we split the texts into list of words).\n",
        "\n",
        "Then for each word of each article, we compute its TF-IDF value.\n",
        "\n",
        "For a term $i$ in a document $j$:\n",
        ">$W_{i,j} = tf_{i,j} * \\log\\frac{N}{df_i}$\n",
        "\n",
        "where:\n",
        "\n",
        ">$tf_{i,j} =$ term frequency of $i$ in $j$\n",
        "\n",
        ">$df_i = $ number of documents containing $i$\n",
        "\n",
        ">$N = $ total number of documents\n"
      ]
    },
    {
      "metadata": {
        "id": "TehPJyiHyRPf",
        "colab_type": "code",
        "outputId": "b00c91db-2872-4f45-fa8d-1b99f224757b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from gensim import corpora, models\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "stemmer = SnowballStemmer('english')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U66VWf0BmIJC",
        "colab_type": "code",
        "outputId": "19147cdf-4a72-497f-cce4-b9287702d64d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "# preprocess text\n",
        "def lemmatize_stemming(text):\n",
        "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
        "def preprocess(text):\n",
        "    result = []\n",
        "    n_sentences = text.count(\". \")\n",
        "    for token in gensim.utils.simple_preprocess(text):\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 1:\n",
        "            result.append(lemmatize_stemming(token))\n",
        "    return result\n",
        "  \n",
        "df[\"tokenized_content\"] = df[\"content\"].map(preprocess)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eid</th>\n",
              "      <th>title</th>\n",
              "      <th>content</th>\n",
              "      <th>label</th>\n",
              "      <th>tokenized_content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PSNTZO8gXFUe+cpCZyApw0vEKWPT4b14D6teBEocIAE=_1...</td>\n",
              "      <td>Facebook sets a new task for AI: guide a virtu...</td>\n",
              "      <td>How do you teach computers to understand langu...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[teach, comput, understand, languag, transcrib...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>PSNTZO8gXFUe+cpCZyApw0vEKWPT4b14D6teBEocIAE=_1...</td>\n",
              "      <td>Appleâ€™s new AI chief might actually be the rig...</td>\n",
              "      <td>What is John Giannandrea, Googleâ€™s former hea...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[john, giannandrea, googl, head, search, ai, g...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>t9NJb6rMt3WVivgF5JJtqOwhhWhdxPuJjlu7LCw1vdk=_1...</td>\n",
              "      <td>Banks are already bumping up against the limit...</td>\n",
              "      <td>While big tech companies might not face regu...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[big, tech, compani, face, regul, artifici, in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>t9NJb6rMt3WVivgF5JJtqOwhhWhdxPuJjlu7LCw1vdk=_1...</td>\n",
              "      <td>Itâ€™s Googleâ€™s turn to ask the questions</td>\n",
              "      <td>At Googleâ€™s annual developer conference this ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[googl, annual, develop, confer, past, week, c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PSNTZO8gXFUe+cpCZyApw0vEKWPT4b14D6teBEocIAE=_1...</td>\n",
              "      <td>The White House has set up a task force to hel...</td>\n",
              "      <td>The White House has set up a new task force d...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[white, hous, set, new, task, forc, dedic, art...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 eid  \\\n",
              "0  PSNTZO8gXFUe+cpCZyApw0vEKWPT4b14D6teBEocIAE=_1...   \n",
              "1  PSNTZO8gXFUe+cpCZyApw0vEKWPT4b14D6teBEocIAE=_1...   \n",
              "2  t9NJb6rMt3WVivgF5JJtqOwhhWhdxPuJjlu7LCw1vdk=_1...   \n",
              "3  t9NJb6rMt3WVivgF5JJtqOwhhWhdxPuJjlu7LCw1vdk=_1...   \n",
              "4  PSNTZO8gXFUe+cpCZyApw0vEKWPT4b14D6teBEocIAE=_1...   \n",
              "\n",
              "                                               title  \\\n",
              "0  Facebook sets a new task for AI: guide a virtu...   \n",
              "1  Appleâ€™s new AI chief might actually be the rig...   \n",
              "2  Banks are already bumping up against the limit...   \n",
              "3            Itâ€™s Googleâ€™s turn to ask the questions   \n",
              "4  The White House has set up a task force to hel...   \n",
              "\n",
              "                                             content  label  \\\n",
              "0  How do you teach computers to understand langu...    1.0   \n",
              "1   What is John Giannandrea, Googleâ€™s former hea...    1.0   \n",
              "2    While big tech companies might not face regu...    1.0   \n",
              "3   At Googleâ€™s annual developer conference this ...    1.0   \n",
              "4   The White House has set up a new task force d...    1.0   \n",
              "\n",
              "                                   tokenized_content  \n",
              "0  [teach, comput, understand, languag, transcrib...  \n",
              "1  [john, giannandrea, googl, head, search, ai, g...  \n",
              "2  [big, tech, compani, face, regul, artifici, in...  \n",
              "3  [googl, annual, develop, confer, past, week, c...  \n",
              "4  [white, hous, set, new, task, forc, dedic, art...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "lR7joeUYpGNI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocab = corpora.Dictionary(df.tokenized_content)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JfSH2BQVtAK3",
        "colab_type": "code",
        "outputId": "6d047487-dcc0-45e1-a154-86c25b248de3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "# apply tfidf model\n",
        "def to_vector(key_value_tuples, vector_dim, default_value=0):\n",
        "    rv = np.ones(vector_dim) * default_value\n",
        "    for key, val in key_value_tuples:\n",
        "        rv[key] = val\n",
        "    return rv\n",
        "  \n",
        "df[\"bow_corpus\"] = df.tokenized_content.map(vocab.doc2bow)\n",
        "tfidf = models.TfidfModel(df.bow_corpus)\n",
        "df[\"tfidf\"] = df.bow_corpus.map(lambda bow: to_vector(tfidf[bow], len(vocab)))\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eid</th>\n",
              "      <th>title</th>\n",
              "      <th>content</th>\n",
              "      <th>label</th>\n",
              "      <th>tokenized_content</th>\n",
              "      <th>bow_corpus</th>\n",
              "      <th>tfidf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PSNTZO8gXFUe+cpCZyApw0vEKWPT4b14D6teBEocIAE=_1...</td>\n",
              "      <td>Facebook sets a new task for AI: guide a virtu...</td>\n",
              "      <td>How do you teach computers to understand langu...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[teach, comput, understand, languag, transcrib...</td>\n",
              "      <td>[(0, 2), (1, 1), (2, 1), (3, 5), (4, 16), (5, ...</td>\n",
              "      <td>[0.03314139276585894, 0.01696869807979925, 0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>PSNTZO8gXFUe+cpCZyApw0vEKWPT4b14D6teBEocIAE=_1...</td>\n",
              "      <td>Appleâ€™s new AI chief might actually be the rig...</td>\n",
              "      <td>What is John Giannandrea, Googleâ€™s former hea...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[john, giannandrea, googl, head, search, ai, g...</td>\n",
              "      <td>[(4, 11), (6, 7), (11, 2), (15, 2), (16, 1), (...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.1501294832085813, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>t9NJb6rMt3WVivgF5JJtqOwhhWhdxPuJjlu7LCw1vdk=_1...</td>\n",
              "      <td>Banks are already bumping up against the limit...</td>\n",
              "      <td>While big tech companies might not face regu...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[big, tech, compani, face, regul, artifici, in...</td>\n",
              "      <td>[(4, 5), (5, 1), (21, 2), (33, 2), (34, 1), (5...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.1298592676820325, 0.049...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>t9NJb6rMt3WVivgF5JJtqOwhhWhdxPuJjlu7LCw1vdk=_1...</td>\n",
              "      <td>Itâ€™s Googleâ€™s turn to ask the questions</td>\n",
              "      <td>At Googleâ€™s annual developer conference this ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[googl, annual, develop, confer, past, week, c...</td>\n",
              "      <td>[(1, 1), (4, 1), (6, 6), (9, 1), (11, 2), (13,...</td>\n",
              "      <td>[0.0, 0.019245058305825548, 0.0, 0.0, 0.018573...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PSNTZO8gXFUe+cpCZyApw0vEKWPT4b14D6teBEocIAE=_1...</td>\n",
              "      <td>The White House has set up a task force to hel...</td>\n",
              "      <td>The White House has set up a new task force d...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[white, hous, set, new, task, forc, dedic, art...</td>\n",
              "      <td>[(4, 14), (20, 1), (53, 1), (69, 1), (76, 1), ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.3362381485574112, 0.0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 eid  \\\n",
              "0  PSNTZO8gXFUe+cpCZyApw0vEKWPT4b14D6teBEocIAE=_1...   \n",
              "1  PSNTZO8gXFUe+cpCZyApw0vEKWPT4b14D6teBEocIAE=_1...   \n",
              "2  t9NJb6rMt3WVivgF5JJtqOwhhWhdxPuJjlu7LCw1vdk=_1...   \n",
              "3  t9NJb6rMt3WVivgF5JJtqOwhhWhdxPuJjlu7LCw1vdk=_1...   \n",
              "4  PSNTZO8gXFUe+cpCZyApw0vEKWPT4b14D6teBEocIAE=_1...   \n",
              "\n",
              "                                               title  \\\n",
              "0  Facebook sets a new task for AI: guide a virtu...   \n",
              "1  Appleâ€™s new AI chief might actually be the rig...   \n",
              "2  Banks are already bumping up against the limit...   \n",
              "3            Itâ€™s Googleâ€™s turn to ask the questions   \n",
              "4  The White House has set up a task force to hel...   \n",
              "\n",
              "                                             content  label  \\\n",
              "0  How do you teach computers to understand langu...    1.0   \n",
              "1   What is John Giannandrea, Googleâ€™s former hea...    1.0   \n",
              "2    While big tech companies might not face regu...    1.0   \n",
              "3   At Googleâ€™s annual developer conference this ...    1.0   \n",
              "4   The White House has set up a new task force d...    1.0   \n",
              "\n",
              "                                   tokenized_content  \\\n",
              "0  [teach, comput, understand, languag, transcrib...   \n",
              "1  [john, giannandrea, googl, head, search, ai, g...   \n",
              "2  [big, tech, compani, face, regul, artifici, in...   \n",
              "3  [googl, annual, develop, confer, past, week, c...   \n",
              "4  [white, hous, set, new, task, forc, dedic, art...   \n",
              "\n",
              "                                          bow_corpus  \\\n",
              "0  [(0, 2), (1, 1), (2, 1), (3, 5), (4, 16), (5, ...   \n",
              "1  [(4, 11), (6, 7), (11, 2), (15, 2), (16, 1), (...   \n",
              "2  [(4, 5), (5, 1), (21, 2), (33, 2), (34, 1), (5...   \n",
              "3  [(1, 1), (4, 1), (6, 6), (9, 1), (11, 2), (13,...   \n",
              "4  [(4, 14), (20, 1), (53, 1), (69, 1), (76, 1), ...   \n",
              "\n",
              "                                               tfidf  \n",
              "0  [0.03314139276585894, 0.01696869807979925, 0.0...  \n",
              "1  [0.0, 0.0, 0.0, 0.0, 0.1501294832085813, 0.0, ...  \n",
              "2  [0.0, 0.0, 0.0, 0.0, 0.1298592676820325, 0.049...  \n",
              "3  [0.0, 0.019245058305825548, 0.0, 0.0, 0.018573...  \n",
              "4  [0.0, 0.0, 0.0, 0.0, 0.3362381485574112, 0.0, ...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "heYG3l0Od0y-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And the vectorization part is done ! We can now use it to train a classifier !"
      ]
    },
    {
      "metadata": {
        "id": "uw4EEBd6d-Yy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train the model\n",
        "\n",
        "We will use a [K Nearest Neighbors classifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) after a [PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)"
      ]
    },
    {
      "metadata": {
        "id": "xON1-dw11loE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2rrUdl7lhDav",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# split into a train set and a test set, so that we can evaluate the model on unseen articles\n",
        "X = np.array(df.tfidf.tolist())\n",
        "y = np.array(df.label.tolist())\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mkTtgISUCzQN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# apply pca\n",
        "pca = PCA()\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZYpW6dmISDhj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# print the words with the highest weight for several principal components\n",
        "words = set()\n",
        "for i in range(len(pca.components_)):\n",
        "  if i>= 30: break\n",
        "  words.add(vocab[max(range(pca.n_components_), key=lambda j: np.abs(pca.components_[i][j]))])\n",
        "print(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g6QVoQ0H66aI",
        "colab_type": "code",
        "outputId": "bae795dd-5fa3-4a97-b0c7-f220e654c888",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# train knn model\n",
        "model = KNeighborsClassifier(n_neighbors=5)\n",
        "model.fit(X_train_pca, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
              "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
              "           weights='uniform')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "hnCgjwjAAtdu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can now compute the accuracy of your classifier. You should see something between 0.80 and 1. Higher is better."
      ]
    },
    {
      "metadata": {
        "id": "Nx7DLD717vtk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# score on train set (accuracy)\n",
        "model.score(X_train_pca, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ldp5FCh6hUe_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# score on test set (accuracy)\n",
        "model.score(X_test_pca, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lM5ITtQngoCG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Your smart assistant is ready to classify new articles ! Let's save new interesting article into one of your boards"
      ]
    },
    {
      "metadata": {
        "id": "z7zvmc2Dg4Ui",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Apply the model on new articles and save them\n",
        "\n",
        "To do so, we will download many articles from your source feed, apply the classifier and then use the `feedly-client` to save them into a board.\n",
        "\n",
        "To save an article to a board named `board_name` you can do as follows:\n",
        "\n",
        "```\n",
        "sess.user.get_tag(board_name).tag_entry(entry_id)\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "FYQiXdjR9Sm5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# download new entries\n",
        "max_count = 300\n",
        "if source_feed is not None:\n",
        "  new_articles = list(sess.user.get_category(source_feed).stream_contents(options=StreamOptions(max_count=max_count)))\n",
        "else:\n",
        "  new_articles = list(Streamable({'id': default_sources_ids}, sess).stream_contents(options=StreamOptions(max_count=max_count)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y1RXbmAs-i7v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess_articles(articles):\n",
        "  df = build_dataframe(articles)\n",
        "  df[\"tokenized_content\"] = df[\"content\"].map(preprocess)\n",
        "  return df\n",
        "def compute_tfidf(df):\n",
        "  df[\"bow_corpus\"] = df.tokenized_content.map(vocab.doc2bow)\n",
        "  df[\"tfidf\"] = df.bow_corpus.map(lambda bow: to_vector(tfidf[bow], len(vocab)))\n",
        "  df[\"X\"] = list(pca.transform(df.tfidf.tolist()))\n",
        "  return df.X.tolist()\n",
        "\n",
        "# process new entries data\n",
        "preprocessed_articles = preprocess_articles(new_articles)\n",
        "tfidf_vectors = compute_tfidf(preprocessed_articles)\n",
        "\n",
        "# apply knn model\n",
        "predictions = model.predict(tfidf_vectors)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w1aUvhRf-mXw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# store predictions\n",
        "new_df = preprocessed_articles\n",
        "new_df[\"pred\"] = predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wi9RRHIb_k3A",
        "colab_type": "code",
        "outputId": "b177c956-0a28-403c-f6b0-9f8af5cca7fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "cell_type": "code",
      "source": [
        "# show results\n",
        "print(f\"Predicted {len(new_df.loc[new_df.pred==1])} positive articles. A few examples:\\n{new_df.loc[new_df.pred==1].head().title}\\n...\")\n",
        "print(f\"Predicted {len(new_df.loc[new_df.pred==0])} negative articles. A few examples:\\n{new_df.loc[new_df.pred==0].head().title}\\n...\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted 14 positive articles. A few examples:\n",
            "17     Google is using AI to help The New York Times ...\n",
            "23     Amazon told employees it would continue to sel...\n",
            "125    China implements tech that can detect people b...\n",
            "160    Microsoft Word is getting a to-do feature to h...\n",
            "182     The way we text says a lot about our personality\n",
            "Name: title, dtype: object\n",
            "...\n",
            "Predicted 286 negative articles. A few examples:\n",
            "0    Google Walkout organizers: changes are a start...\n",
            "1    The fake video era of US politics has arrived ...\n",
            "2    Tesla's Model 3 gets quicker cornering with 'T...\n",
            "3    New Apple patent hints that rumored over-ear h...\n",
            "4    BMWâ€™s i8 Roadster is a daily driver in superca...\n",
            "Name: title, dtype: object\n",
            "...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kmpcaNH-yHGx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So, what do you think about the results ? They really depend on the data the classifier is trained on, but in general you should see that the topics of the predicted positive articles are close to the topic of your positive board.\n",
        "\n",
        "**Feel free to try with other source feeds and boards !**\n",
        "\n",
        "You can do that by changing the `source_feed` and `positive_board` values and rerun the notebook."
      ]
    },
    {
      "metadata": {
        "id": "P6QJ-3WAh-xP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you are happy with the results, you can enter the board name in which you want the new articles to be saved:"
      ]
    },
    {
      "metadata": {
        "id": "xvT4iR98WDIr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Here are all your boards:\")\n",
        "for tag_name in sess.user.get_tags():\n",
        "  print(\"\\t\",f'\"{tag_name}\"')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jj0MmsqesWwn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO: Enter your output board, where the new positive articles will be saved.\n",
        "# If the board doesn't exist, a new board will be created for you.\n",
        "output_board = None\n",
        "\n",
        "if output_board is None:\n",
        "  raise ValueError(\"Please enter a board name\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BvGwA7cS_yvG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# tag the articles\n",
        "\n",
        "def get_positive_predictions(df):\n",
        "  return df.loc[new_df.pred==1]\n",
        "\n",
        "tag = sess.user.get_tag(output_board)\n",
        "output_articles = get_positive_predictions(new_df)\n",
        "\n",
        "for i,eid in enumerate(output_articles.eid):\n",
        "  title = output_articles.iloc[i].title\n",
        "  tag.tag_entry(eid)\n",
        "  print(f\"Saved '{title}' into the board '{output_board}'\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GBoJU_QriDVJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can now check your feedly to see the new articles saved into your board ! Feel free to build different classifiers for different topics/positive boards"
      ]
    },
    {
      "metadata": {
        "id": "U7N3sx50_npg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note for the developers: the feedly APIs rate is limited to 250 API requests per day (500 requests per day for feedly pro and team accounts)."
      ]
    },
    {
      "metadata": {
        "id": "b0BRvuafz554",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Next\n",
        "\n",
        "You can now go further and build complex datasets on feedly on topics, trends... and use a better model:\n",
        "- **Extract other features** from the articles, depending on you task (keywords detection, entity recognition, word/sentence embeddings, article size... )\n",
        "- **Use other classifiers** instead of KNN: naive bayes, SVM, decision trees, neural networks..."
      ]
    },
    {
      "metadata": {
        "id": "8sFmAEwpAN-Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Thank you !\n",
        "\n",
        "Congratulations, you just build yourself a smart assistant for your feedly account !\n",
        "Feel free to give feedbacks in the comments section of the feedly blog post, or you can also email me at quentin@feedly.com (or at lhoest.q@gmail.com)"
      ]
    },
    {
      "metadata": {
        "id": "eqGoJrzOCvXO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Passionate about the Web, NLP and Machine Learning? [Join the Feedly Lab on Slack](https://join.slack.com/t/feedlylab/shared_invite/enQtNDEyMzQ2Nzk4OTQ1LWQ3ZmExOWYwNDUwN2U4Yzg2MDZjNDJmZDQ4YTFiY2RmYjIyMjBmOGZiZDQwODQxZjRiZDY2Mzc1NTc1YjNjMmQ) and connect with the Feedly machine learning team!"
      ]
    },
    {
      "metadata": {
        "id": "RFKEZbFQmZTd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Keep reading !\n",
        "\n",
        "- [Getting started with Machine Learning](https://blog.feedly.com/getting-started-with-machine-learning/) <- a list of many good references\n",
        "\n",
        "- [Transfer Learning in NLP](https://blog.feedly.com/transfer-learning-in-nlp/) <- advanced transfer learning techniques to improve your NLP model"
      ]
    },
    {
      "metadata": {
        "id": "AfUqBeSaATfV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}