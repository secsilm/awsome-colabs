{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project 4",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/secsilm/awsome-colabs/blob/master/UCBerkeley-cs228-assignments/Project_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeUOYktpXtOz"
      },
      "source": [
        "# Project 4: Semantic Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFEIwsqI7YKR"
      },
      "source": [
        "This project will have you implement a neural semantic parser for the GeoQA dataset of [Krishnamurthy and Kollar, 2013](http://rtw.ml.cmu.edu/tacl2013_lsp/tacl2013-krishnamurthy-kollar.pdf), which consists of a database of simple geographic facts about 10 US states, questions and answers about the database, and annotated logical forms for the questions. Your final system will go from natural language questions to their answers, computed from the database, via logical forms that are executed on the database. \n",
        "\n",
        "First, you'll implement a method for executing logical forms on the database. Then, you'll implement some components of a constrained sequence-to-sequence model for producing logical forms from questions. You will train it using paired questions and logical forms. Then, you will train it from questions-answer pairs, by searching over latent logical forms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEF2PcoBc97Q"
      },
      "source": [
        "Note: this dataset is small enough that we will be able to train on CPU; you don't need a GPU instance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acPh_4GwYID0"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7cggO7mjZ5L"
      },
      "source": [
        "The dependencies for this project include:\n",
        "* `torch` for modeling and training\n",
        "* `sexpdata` for loading logical forms from [S-expressions](https://en.wikipedia.org/wiki/S-expression)\n",
        "* `geoqa`: support code for loading and preprocessing the GeoQA database, evaluating predicted answers, and support code, available at this [github repo](https://github.com/dpfried/geoqa-release) (although you can treat the code as a black box and not worry about the details)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjPOhBQs6-IQ"
      },
      "source": [
        "%%capture\n",
        "!pip install --upgrade torch tqdm sexpdata\n",
        "# this provides the packages available here: https://github.com/dpfried/geoqa-release\n",
        "!wget https://github.com/dpfried/geoqa-release/archive/master.zip -O geoqa.zip\n",
        "!unzip geoqa.zip\n",
        "!rm geoqa.zip\n",
        "!mv geoqa-release-master/data.tgz .\n",
        "!mv geoqa-release-master/geoqa .\n",
        "!mv geoqa-release-master/vecs .\n",
        "!rm -r geoqa-release-master\n",
        "!tar xf data.tgz\n",
        "\n",
        "# Standard library imports\n",
        "import math\n",
        "import random\n",
        "import pprint\n",
        "import pickle\n",
        "from typing import Union, List, Set, Tuple\n",
        "from collections import namedtuple\n",
        "\n",
        "# Third party imports\n",
        "import editdistance\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "import torch.nn as nn\n",
        "import tqdm.notebook\n",
        "import sexpdata\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# geoqa imports; we'll examine these shortly\n",
        "import geoqa\n",
        "import geoqa.geo\n",
        "import geoqa.utils\n",
        "\n",
        "from geoqa.utils import Stack, Index, logical_form_to_str\n",
        "from geoqa.geo import STATES, World\n",
        "from geoqa.geo import CATS, RELS, ENTITY_ACTIONS_TO_ENTITIES\n",
        "from geoqa.geo import LAMBDA, EXISTS, AND, VARS, MAX_LITERALS\n",
        "from geoqa.dataset import GeoDataset\n",
        "from geoqa.evaluation import evaluate_predictions\n",
        "\n",
        "BIG_NEG = -1e9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOcjFTuVYI6C"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv81fe200KlE"
      },
      "source": [
        "First, let's examine the dataset. GeoQA contains 10 databases, one for each of 10 states."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uPT-oz40gYQ"
      },
      "source": [
        "STATES"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLv68jB2y6Rc"
      },
      "source": [
        "DEV_STATES = [\"va\", \"wv\"]\n",
        "TEST_STATES = [\"fl\", \"ga\"]\n",
        "TRAIN_STATES = [env for env in STATES if env not in DEV_STATES + TEST_STATES]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_zKfbxB4sHC"
      },
      "source": [
        "A database consists of a set of entities, and a collection of facts about these entities. Facts are either categories, which are unary predicates on entities, or relations, which are binary predicates between entity pairs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "131kc8aj5ySe"
      },
      "source": [
        "# all possible categories that can be used across states\n",
        "' '.join(CATS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeDYWJ_k52YP"
      },
      "source": [
        "# all possible relations that can be used across states\n",
        "' '.join(RELS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeznyh2p55d7"
      },
      "source": [
        "Not all categories and relations will be used in a given database; but here are a representation of the entities, categories, and relations for one state. This dataset is quite small, and the state data below represents one of the larger databases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJHntYwQ1XVs"
      },
      "source": [
        "world = geoqa.geo.read_world(\"va\")\n",
        "world.print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Vn6mqMc0nio"
      },
      "source": [
        "Each state has a set of question-answer pairs, with an associated logical form for each question."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1LU0bazywMw"
      },
      "source": [
        "questions, answers, logical_forms = geoqa.geo.read_data(\"va\")\n",
        "\n",
        "instance_index = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAyAMRVf0x8r"
      },
      "source": [
        "questions[instance_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT4WajPfGHdk"
      },
      "source": [
        "Each answer is a set of entities from the database:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuW8g2E500VF"
      },
      "source": [
        "answers[instance_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXDjQFKZ067q"
      },
      "source": [
        "Logical forms are lambda calculus functions represented as tuples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sF0G35Hz04-L"
      },
      "source": [
        "logical_forms[instance_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AdTo5zkA_pF"
      },
      "source": [
        "print(logical_form_to_str(logical_forms[instance_index]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru5yVIOG6U9t"
      },
      "source": [
        "This dataset has fairly simple lambda calculus expressions, which always consist of a single function and a conjunction of predicates. Because of this, every logical form contains:\n",
        "1. **Variables** (`$x, $y, $z,` and `$w`), which can take on entity values from the database.\n",
        "2. **Predicates**, of which there are three types:\n",
        "    - **categories** (e.g. `city`), which take 1 variable as an argument, and return true if the category is true in the database for the entity assigned to the variable.\n",
        "    - **relations** (e.g. `in-rel`), which take 2 variable arguments, and return true if the relation is true in the database for the ordered pair of entities assigned to the variables.\n",
        "    - **entity predicates** (e.g. `kb-virginia`), which takes 1 variable argument and return true if the proper entity (e.g. `virginia`) is assigned to the variable.\n",
        "3. A **conjunction** (`and`) of multiple **literals** (e.g. `(city $w)` is a literal). A literal consists of a predicate with some number of variables as argument.\n",
        "4. [optional] An **existential quantifier** (`exists`), which takes as arguments all but one of the variables used within the conjunction, and the conjunction. `exists` is used if and only if more than one variable is used inside the `and` expression.\n",
        "5. A **lambda expression** (`lambda`), which takes as arguments a single variable and either a conjunction (if the conjunction contains no other variables, other than the lambda's argument) or an existential quantifier (if it does).\n",
        "\n",
        "Note: an alternative, more compact logical form representation would use entities directly when applicable, e.g. `(lambda $w (and (city $w) (in-rel $w virginia)))`. This would make the implementation of the logical form executor (which you'll complete in the next section) a bit more complex, but it would likely reduce the difficulty of learning the logical forms' structure. If you're interested, see [Liang 2013](https://arxiv.org/abs/1309.4408) for one approach used in many recent systems which takes this even further to eliminate all variables and make existential quantification implicit, more closely paralleling natural language.\n",
        "\n",
        "In general, semantic parsing systems must trade off between the complexity of the lexicon, the syntax/semantics interface, compactness of the logical form structure, and the difficulty of inference and learning.  For this dataset, we'll find that a neural model is able to adequately learn  to predict these relatively verbose logical forms directly from sentences, given some lexical and structural constraints."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnm8zv7jMhtx"
      },
      "source": [
        "# obtain all predicates (categories, relations, and entity predicates) from the dataset\n",
        "PREDICATES = set(CATS) | set(RELS) | set(ENTITY_ACTIONS_TO_ENTITIES.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm5aJW-zMsw1"
      },
      "source": [
        "' '.join(sorted(PREDICATES))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnbgAnhEFraV"
      },
      "source": [
        "## Logical Form Executor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeTINkBfF68I"
      },
      "source": [
        "All logical forms in this dataset are lambda functions, which take database entities and return truth values. Each logical form's denotation (corresponding to the answer to its question) is the set of entities in the database for which the lambda function is true."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxRwckgWF8_G"
      },
      "source": [
        "questions[instance_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQTXYMTrGdXc"
      },
      "source": [
        "logical_form_to_str(logical_forms[instance_index])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTJD-e7k12Pq"
      },
      "source": [
        "answers[instance_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYsQsGbOeX3G"
      },
      "source": [
        "To evaluate a logical form's denotation in a database, we'll run a simple unification algorithm to build a denotation up recursively from sub-trees in the logical form's tree. Each sub-tree in the logical form will be associated with a `VariableAssignments` object, which contains the set of all possible assignments of entities to variables within the sub-tree that would satisfy the logical form:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5Qn0IGBQyxD"
      },
      "source": [
        "# VariableAssignments: a collection of possible assignments of entities to\n",
        "#   variables that satisfy the logical form, as processed so far\n",
        "VariableAssignments = namedtuple('VariableAssignments', [\n",
        "  'variables',\n",
        "  # variables: Set[str]: variables that are assigned\n",
        "  'assignments'\n",
        "  # assignments: List[Dict[str, str]]: a list of variable assignments.\n",
        "  #   each variable assignment dictionary should map every variable in \n",
        "  #   `variables` to an entity\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n28xqRMBfhdY"
      },
      "source": [
        "In the cell below, we've defined operations to produce satisfying assignments for each of the three literal types (which occur at the leaves of the tree):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5re9oORVQu0V"
      },
      "source": [
        "def entity_op(entity_predicate: str, variable: str, world: World) -> VariableAssignments:\n",
        "  entity_name = ENTITY_ACTIONS_TO_ENTITIES[entity_predicate]\n",
        "  assert entity_name in world.entities\n",
        "  return VariableAssignments({variable}, [{variable: entity_name}])\n",
        "\n",
        "def category_op(category: str, variable: str, world: World) -> VariableAssignments:\n",
        "  index = CATS.index(category)\n",
        "  values = (world.categories[index] == 1).nonzero()[0]\n",
        "  return VariableAssignments(\n",
        "    {variable},\n",
        "    [{variable: world.index_to_entities[value]}\n",
        "     for value in values],\n",
        "  )\n",
        "\n",
        "def relation_op(relation: str, variable_1: str, variable_2: str, world: World) -> VariableAssignments:\n",
        "  index = RELS.index(relation)\n",
        "  val_1s, val_2s = (world.relations[index] == 1).nonzero()\n",
        "  assignments = [\n",
        "    {variable_1: world.index_to_entities[val_1], variable_2: world.index_to_entities[val_2]}\n",
        "    for val_1, val_2 in zip(\n",
        "      val_1s, val_2s\n",
        "    )\n",
        "  ]\n",
        "  return VariableAssignments(\n",
        "    {variable_1, variable_2},\n",
        "    assignments\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfEmeVUWg1tp"
      },
      "source": [
        "These literal operations are demonstrated below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToUxQn47StAm"
      },
      "source": [
        "world = geoqa.geo.read_world(\"wv\")\n",
        "world.print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znHyqHw5SwKR"
      },
      "source": [
        "logical_form = geoqa.utils.parse_tree('(lambda $w (exists $x (and (city $w) (in-rel $w $x) (kb-west_virginia $x))))')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BZRKtM-Sm2I"
      },
      "source": [
        "entity_op('kb-west_virginia', '$x', world)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UrNfdWZTqKL"
      },
      "source": [
        "relation_op('in-rel', '$w', '$x', world)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFwXqlpKTyJ-"
      },
      "source": [
        "category_op('city', '$w', world)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp8ZwdbKf7J6"
      },
      "source": [
        "In the cell below, we've defined an `execute` function which will return the entity set denotation for a logical form by recursively traversing the logical form's tree and combining variable assignments from the leaves upward, using _sub-tree operations_.  The root node of the tree, the `lambda` node, has a `lambda_op` sub-tree operation which takes a `VariableAssignments` containing possible satisfying assignments, and returns the set of entities that the lambda variable takes on.\n",
        "\n",
        "All other sub-tree operations take `VariableAssignments` returned by their children, and return a `VariableAssignments` giving the assignments that satisfy the sub-tree.\n",
        "\n",
        "Some of the sub-tree operations (`entity_op`, `category_op`, `relation_op`) for literals were defined above; you'll define the others (`lambda_op`, `exists_op`, `and_op`) in the cells afterward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18BLQZKCVBl1"
      },
      "source": [
        "def execute(logical_form: Tuple, world: World):\n",
        "  # recursively process the tree from the leaves upward. Each subtree should return:\n",
        "  #   lambda nodes: a `set` of entities that satisfy the logical form\n",
        "  #   all other node types: a VariableAssignments that contains possible assignments\n",
        "  #       of entities to variables that would satisfy the subtree\n",
        "  assert isinstance(logical_form, tuple)\n",
        "  predicate = logical_form[0]\n",
        "  if predicate in ENTITY_ACTIONS_TO_ENTITIES:\n",
        "    var = logical_form[1]\n",
        "    return entity_op(predicate, var, world)\n",
        "  elif predicate in CATS:\n",
        "    category = predicate\n",
        "    var = logical_form[1]\n",
        "    return category_op(category, var, world)\n",
        "  elif predicate in RELS:\n",
        "    rel = predicate\n",
        "    var_1, var_2 = logical_form[1:]\n",
        "    return relation_op(rel, var_1, var_2, world)\n",
        "  elif predicate == LAMBDA:\n",
        "    var = logical_form[1]\n",
        "    var_assignments = execute(logical_form[2], world)\n",
        "    return lambda_op(var, var_assignments)\n",
        "  elif predicate == EXISTS:\n",
        "    vars = logical_form[1:-1]\n",
        "    var_assignments = execute(logical_form[-1], world)\n",
        "    return exists_op(vars, var_assignments)\n",
        "  elif predicate == AND:\n",
        "    return and_op([\n",
        "      execute(sub_lf, world)\n",
        "      for sub_lf in logical_form[1:]\n",
        "    ])\n",
        "  else:\n",
        "    raise NotImplementedError(\"invalid predicate {}\\n{}\".format(predicate, logical_form))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JpGURQbX8IG"
      },
      "source": [
        "# helper function to use when testing results from the sub-tree operations\n",
        "def are_equal(va1: VariableAssignments, va2: VariableAssignments) -> bool:\n",
        "  def canonicalize(assignments):\n",
        "    # compare sets of assignments, since ordering (and duplication) of assignments within the list does not affect meaning\n",
        "    # convert each assignment dict to a string in json format since dictionaries are unhashable\n",
        "    import json\n",
        "    return set(json.dumps(d) for d in assignments)\n",
        "  return va1.variables == va2.variables and canonicalize(va1.assignments) == canonicalize(va2.assignments)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUiGL5FKmajc"
      },
      "source": [
        "Complete the `lambda_op`, `exists_op`, and `and_op` sub-tree operations in the cells below. Each operation has a test cell after it with sample inputs and outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP6LrhT5U4iq"
      },
      "source": [
        "def lambda_op(lambda_variable: str, variable_assignments: VariableAssignments) -> Set[str]:\n",
        "  # return the entities that variable takes on in variable_assignments\n",
        "  \"\"\"YOUR CODE HERE\"\"\"\n",
        "  \n",
        "  ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mZMhJ4cVr_v"
      },
      "source": [
        "assert lambda_op('$y', VariableAssignments(\n",
        "    {'$x', '$y'}, \n",
        "    [{'$x': 'virginia', '$y': 'richmond'}, {'$x': 'virginia', '$y': 'charleston'}]\n",
        "  )) == {'richmond', 'charleston'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XUNvm9PVh-8"
      },
      "source": [
        "def exists_op(existentially_quantified_vars: List[str], child_assignments: VariableAssignments) -> VariableAssignments:\n",
        "  # remove the existentially-quantified variables from the assignments in variable_assignments\n",
        "  \"\"\"YOUR CODE HERE\"\"\"\n",
        "\n",
        "  ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1n76aJCWU3U"
      },
      "source": [
        "exists_test_1 = exists_op(['$x'], VariableAssignments(\n",
        "    {'$x', '$y'}, [{'$x': 'virginia', '$y': 'richmond'}, {'$x': 'virginia', '$y': 'charleston'}]\n",
        "  ))\n",
        "assert are_equal(exists_test_1, VariableAssignments(\n",
        "    {'$y'}, [{'$y': 'richmond'}, {'$y': 'charleston'}]\n",
        ")), exists_test_1\n",
        "                                             \n",
        "exists_test_2 = exists_op(['$y'], VariableAssignments(\n",
        "    {'$x', '$y'}, [{'$x': 'virginia', '$y': 'richmond'}, {'$x': 'virginia', '$y': 'charleston'}]\n",
        "  ))\n",
        "assert are_equal(exists_test_2, VariableAssignments(\n",
        "    {'$x'},\n",
        "    [{'$x': 'virginia'}]\n",
        ")), exists_test_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPtKal5MWRge"
      },
      "source": [
        "def and_op(children_assignments: List[VariableAssignments]) -> VariableAssignments:\n",
        "  # join the assignments in children_assignments, to return possible assignments that satisfy all children\n",
        "  \"\"\"YOUR CODE HERE\"\"\"\n",
        "\n",
        "  ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmsZOYpHWSmX"
      },
      "source": [
        "and_test = and_op([\n",
        "  VariableAssignments({'$x', '$y'}, [{'$x': 'x1', '$y': 'y1'}, {'$x': 'x2', '$y': 'y2'}]),\n",
        "  VariableAssignments({'$z', '$y'}, [{'$z': 'z2', '$y': 'y2'}, {'$z': 'x3', '$y': 'y3'}]),                 \n",
        "])\n",
        "assert are_equal(\n",
        "  and_test,\n",
        "  VariableAssignments(variables={'$x', '$y', '$z'}, assignments=[{'$x': 'x2', '$y': 'y2', '$z': 'z2'}])\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4H9l75ZmjD9"
      },
      "source": [
        "With correct sub-tree operation definitions, the following test cell should run without errors. It checks that the entity set denotations returned by the `execute` function match the answers in the database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tsh1HyWRQUwe"
      },
      "source": [
        "for state in STATES:\n",
        "  world = geoqa.geo.read_world(state)\n",
        "  for question, answer, logical_form in zip(*geoqa.geo.read_data(state)):\n",
        "    if logical_form is None:\n",
        "      continue\n",
        "    true_denotation = world.get_denotation_from_answer(answer)\n",
        "    executed_denotation = execute(logical_form, world)\n",
        "    if true_denotation != executed_denotation:\n",
        "      print(\"execution failure!\")\n",
        "      print(\"question: {}\".format(question))\n",
        "      print(\"state: {}\".format(world))\n",
        "      print(\"logical form: {}\".format(logical_form))\n",
        "      print(\"true denotation: {}\".format(true_denotation))\n",
        "      print(\"executed denotation: {}\".format(executed_denotation))\n",
        "      raise ValueError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR22RXa8VHtf"
      },
      "source": [
        "## Parser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrlpUdpUg9VK"
      },
      "source": [
        "Now that we have a method to execute logical forms on the database, we'll spend the rest of this assignment constructing parsers to produce logical forms from questions. \n",
        "\n",
        "Our question-to-logical-form parser will be a neural sequence-to-sequence model that will encode the question and decode a sequence of actions to construct a logical form. We will use _constrained decoding_ to prevent the model from constructing invalid logical forms, enforcing structural constraints rather than relying on the model to learn them.\n",
        "\n",
        "To construct this parser, we first need to define a transition system: a set of actions that build up a logical form incrementally, and the effects each action has on a partially constructed logical form. This transition system will constrain the available actions that can be taken by the sequence-to-sequence model at any point in time to guarantee valid logical forms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EInORycTgi2o"
      },
      "source": [
        "### Transition system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-qedMk3IVUm"
      },
      "source": [
        "We'll construct logical forms using a depth-first, left-to-right traversal of the logical form's tree structure, with actions in a post-fix notation (so a predicate comes after the arguments it takes), and a final `DONE` action after the logical form is constructed. For example, the logical form\n",
        "\n",
        "`(lambda $x (exists $w (and (state $w) (city $x) (in-rel $x $w))))`\n",
        "\n",
        "will be constructed in the traversal order:\n",
        "\n",
        "`$w state $x city $x $w in-rel and $w exists $x lambda DONE`\n",
        "\n",
        "We will basically have one action for each of the items in the order above, with one change to simplify what our model needs to learn. In this dataset, which always combines literals with a single `and`, each logical form has exactly one variable as an argument to the lambda expression, and existentially quantifies all other variables. Because of this, the actions between `and` and the `lambda` are determined by the lambda's variable argument (`$x`) and the other variables that have previously been introduced (all variables other than the lambda variable must be existentially quantified). This allows us to replace these actions with a single action, `complete_lambda_$x`. For the example above:\n",
        "\n",
        "`$w state $x city $x $w in-rel complete_lambda_$x DONE`\n",
        "\n",
        "If the question had been about the variable `$w`, we would instead use `complete_lambda_$w` so that `$w` was used with `lambda` and everything else was existentially quantified.\n",
        "\n",
        "Note that other datasets with more complex logical forms would need to generate conjunctions and quantifiers individually, rather than using these simplifying `complete_lambda_` actions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnBDhZ9pgvVL"
      },
      "source": [
        "### Actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCFQbdAR2wrr"
      },
      "source": [
        "We'll now define the actions used by our transition system. Variables and predicates will be taken directly from the items in the logical forms. We'll define new actions for `COMPLETE_LAMBDA_*` and `DONE`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsqClVZyMRhB"
      },
      "source": [
        "# mapping from complete lambda action to the lambda's variable\n",
        "COMPLETE_LAMBDA_ACTIONS_TO_VARS = {'COMPLETE_LAMBDA_{}'.format(var): var for var in VARS}\n",
        "# and reverse\n",
        "VARS_TO_COMPLETE_LAMBDA_ACTIONS = {v: k for k, v in COMPLETE_LAMBDA_ACTIONS_TO_VARS.items()}\n",
        "\n",
        "DONE_ACTION = 'DONE'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xgGUZG43G97"
      },
      "source": [
        "Actions will be divided into three types, which will be produced differently by our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3AjULiM3FLm"
      },
      "source": [
        "STRUCTURAL_ACTIONS = VARS + sorted(COMPLETE_LAMBDA_ACTIONS_TO_VARS.keys()) + [DONE_ACTION]\n",
        "CROSS_DATABASE_PREDICATE_ACTIONS = CATS + RELS\n",
        "DATABASE_SPECIFIC_PREDICATE_ACTIONS = list(ENTITY_ACTIONS_TO_ENTITIES.keys())\n",
        "\n",
        "ACTIONS = set(STRUCTURAL_ACTIONS) | set(CROSS_DATABASE_PREDICATE_ACTIONS) | set(ENTITY_ACTIONS_TO_ENTITIES.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEIUQWe-M65n"
      },
      "source": [
        "print(STRUCTURAL_ACTIONS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMRHADEpNDOf"
      },
      "source": [
        "print(CROSS_DATABASE_PREDICATE_ACTIONS[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMtdIkh0NFwy"
      },
      "source": [
        "print(DATABASE_SPECIFIC_PREDICATE_ACTIONS[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndUycozNgevP"
      },
      "source": [
        "### From actions to logical forms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KDdWJmx6mu1"
      },
      "source": [
        "This `ParseConstraints` class will contain possible options to constrain the logical form being constructed, which will become important later when we begin searching over logical forms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAWHIRbZ6U5v"
      },
      "source": [
        "ParseConstraints = namedtuple('ParseConstraints', [\n",
        "  'possible_predicates',\n",
        "  # set[str]: set of predicates to allow in this logical form\n",
        "  'max_vars',\n",
        "  # int: maximum number of distinct variables to allow in the logical form\n",
        "  #   the maximum number in the dataset is 4 (len(VARS))\n",
        "  'no_repeated_literals',\n",
        "  # bool: if True, don't allow the same literal to appear more than once\n",
        "  #   e.g. (and (state $x) (state $x)) is disallowed, but (and (state $x) (state $y)) is ok\n",
        "  'max_literals'\n",
        "  # int: disallow more than this many literals\n",
        "  #   e.g. if max_literals == 1, (and (state $x) (city $y)) is disallowed\n",
        "  #   the maximum number in the dataset is 7 (MAX_LITERALS)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1ZkyQG3RR-q"
      },
      "source": [
        "To build up a logical form via a sequence of actions, we define a `ParserState` class several blocks below. This will represent the logical form as constructed so far and compute valid actions that can be taken to continue constructing it. The `_ParserState` class here defines the actual state variables, then you will fill in the methods in `ParserState` (without the underscore)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tMTiwEVVAok"
      },
      "source": [
        "_ParserState = namedtuple('_ParserState', [\n",
        "  'variable_argument_stack',\n",
        "  # Stack[str]: variables to be put into the current literal\n",
        "  'completed_literal_stack',\n",
        "  # Stack[tuple]: all literals produced so far\n",
        "  'has_lambda',\n",
        "  # bool: whether the lambda has been generated\n",
        "  'lambda_var',\n",
        "  # str or None: the lambda's variable argument, if lambda has been generated; or None otherwise\n",
        "  'is_complete',\n",
        "  # bool: whether the DONE action has been generated\n",
        "  'all_vars_introduced',\n",
        "  # Stack[str]: variables should be appened to this each time they are used\n",
        "  'past_actions',\n",
        "  # Stack[str]: all previous actions taken\n",
        "  'parse_constraints',\n",
        "  # ParseConstraints: options to constrain the logical form being constructed\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4h2y5lnKFKjn"
      },
      "source": [
        "`variable_argument_stack` contains any variables that  will be put into the literal which is currently being constructed. This is reset to empty when a literal is completed (by a predicate action). All literals, as they are completed, are added to `completed_literal_stack`. This should not be emptied at any point, as it will be used to construct the final logical form."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q43L-UdsAcNU"
      },
      "source": [
        "For example, when generating the logical form \n",
        "\n",
        "`(lambda $x (exists $w (and (state $w) (city $x) (in-rel $x $w)))`\n",
        "\n",
        "using the action sequence\n",
        "\n",
        "`$w state $x city $x $w in-rel complete_lambda_$x DONE`,\n",
        "\n",
        "the `ParserState` will look as follows after the first `$x` action:\n",
        "```\n",
        "ParserState(\n",
        "  variable_argument_stack=['$x'],\n",
        "  completed_literal_stack=[('state', '$w')],\n",
        "  has_lambda=False,\n",
        "  lambda_var=None,\n",
        "  is_complete=False,\n",
        "  all_vars_introduced=['$w', '$x'],\n",
        "  past_actions=['$w', 'state', '$x'],\n",
        "  parse_constraints=...\n",
        ")\n",
        "```\n",
        "and as follows after the `DONE` action:\n",
        "```\n",
        "ParserState(\n",
        "  variable_argument_stack=[],\n",
        "  completed_literal_stack=[('state', '$w'), ('city', '$x'), ('in-rel', '$x', '$w')],\n",
        "  has_lambda=True,\n",
        "  lambda_var=$x,\n",
        "  is_complete=True,\n",
        "  all_vars_introduced=['$w', '$x', '$x', '$w'],\n",
        "  past_actions=['$w', 'state', '$x', 'city', '$x', '$w', 'in-rel', 'COMPLETE_LAMBDA_$x', 'DONE'],\n",
        "  parse_constraints=...\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2G26yqrEy67"
      },
      "source": [
        "We'll now define a `ParserState` wrapper class with methods. Complete the `take_action` method in the `ParserState` class below.\n",
        "\n",
        "Implementation tip: `ParserState` objects are immutable, and the `take_action` method should return a new `ParserState` object, rather than attempting to update the object (this will allow `ParserState` objects to be used in beam search). The [Stack](https://github.com/dpfried/geoqa-release/blob/master/geoqa/utils.py#L7) class used in `ParserState` for `variable_argument_stack`, `completed_literal_stack`, `all_vars_introduced`, and `past_actions` is an immutable data structure that is essentially like a list, but will allow sharing some memory across multiple `ParserState`s as search is performed, for efficiency and to prevent copying. See the cell below for a demonstration:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qyIzf0Pn9-c"
      },
      "source": [
        "# demonstration of the Stack class used in _ParserState\n",
        "actions = Stack.empty().append('$w')\n",
        "print(actions.tolist())\n",
        "actions_plus_city = actions.append('city')\n",
        "actions_plus_state = actions.append('state')\n",
        "print(actions.tolist()) # hasn't changed\n",
        "print(actions_plus_city.tolist()) # shares '$w' with actions\n",
        "print(actions_plus_state.tolist()) # shares '$w' with actions\n",
        "print(actions_plus_city.size) # 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eViX6RM4BiY7"
      },
      "source": [
        "class ParserState(_ParserState):\n",
        "  \"\"\"\n",
        "  Wrapper class to add methods and constants to _ParserState\n",
        "  \"\"\"\n",
        "  PREDICATE_ARITIES = {}\n",
        "  for cat in CATS:\n",
        "    PREDICATE_ARITIES[cat] = 1\n",
        "  for rel in RELS:\n",
        "    PREDICATE_ARITIES[rel] = 2\n",
        "  for entity in ENTITY_ACTIONS_TO_ENTITIES.keys():\n",
        "    PREDICATE_ARITIES[entity] = 1\n",
        "\n",
        "  MAX_ARITY = max(PREDICATE_ARITIES.values())\n",
        "\n",
        "  @staticmethod\n",
        "  def initial_state(parse_constraints):\n",
        "    return ParserState(\n",
        "      variable_argument_stack=Stack.empty(),\n",
        "      completed_literal_stack=Stack.empty(),\n",
        "      has_lambda=False,\n",
        "      lambda_var=None,\n",
        "      is_complete=False,\n",
        "      all_vars_introduced=Stack.empty(),\n",
        "      past_actions=Stack.empty(),\n",
        "      parse_constraints=parse_constraints,\n",
        "    )\n",
        "\n",
        "  def take_action(self, action_symbol: str):\n",
        "    assert action_symbol in ACTIONS, \"invalid action {}\".format(action_symbol)\n",
        "    assert not self.is_complete\n",
        "\n",
        "    assert action_symbol in self.valid_actions(), \"trying to take {} but only valid actions are {}.\\npast actions: {}\".format(\n",
        "      action_symbol, self.valid_actions(), ' '.join(self.actions()))\n",
        "\n",
        "    # initialize all variables that might be updated\n",
        "    past_actions = self.past_actions\n",
        "    has_lambda = self.has_lambda\n",
        "    is_complete = self.is_complete\n",
        "    completed_literal_stack = self.completed_literal_stack\n",
        "    variable_argument_stack = self.variable_argument_stack\n",
        "    all_vars_introduced = self.all_vars_introduced\n",
        "    lambda_var = self.lambda_var\n",
        "\n",
        "    past_actions = past_actions.append(action_symbol)\n",
        "\n",
        "    if action_symbol in PREDICATES:\n",
        "      \"\"\"YOUR CODE HERE\"\"\"\n",
        "\n",
        "      ...\n",
        "    elif action_symbol in VARS:\n",
        "      \"\"\"YOUR CODE HERE\"\"\"\n",
        "\n",
        "      ...\n",
        "    elif action_symbol in COMPLETE_LAMBDA_ACTIONS_TO_VARS.keys():\n",
        "      \"\"\"YOUR CODE HERE\"\"\"\n",
        "\n",
        "      ...\n",
        "    elif action_symbol == DONE_ACTION:\n",
        "      \"\"\"YOUR CODE HERE\"\"\"\n",
        "\n",
        "      ...\n",
        "    else:\n",
        "      raise ValueError(\"invalid action {}\".format(action_symbol))\n",
        "\n",
        "    return ParserState(\n",
        "      variable_argument_stack=variable_argument_stack,\n",
        "      completed_literal_stack=completed_literal_stack,\n",
        "      has_lambda=has_lambda,\n",
        "      lambda_var=lambda_var,\n",
        "      is_complete=is_complete,\n",
        "      all_vars_introduced=all_vars_introduced,\n",
        "      past_actions=past_actions,\n",
        "      parse_constraints=self.parse_constraints,\n",
        "    )\n",
        "\n",
        "  def variables_introduced(self) -> Set[str]:\n",
        "    \"\"\"\n",
        "    :return: variables that have been used in the logical form, as constructed so far\n",
        "    \"\"\"\n",
        "    variables = set(self.all_vars_introduced.tolist())\n",
        "    assert variables == set(VARS[:len(\n",
        "      variables)]), \"variables should not be introduced out of the sequential order {}, but {} were used\".format(VARS,\n",
        "                                                                                                                 vars)\n",
        "    return variables\n",
        "\n",
        "  def variables_usable(self) -> Set[str]:\n",
        "    \"\"\"\n",
        "    To reduce the set of possible logical forms, enforce that new variables are introduced in alphabetical order, e.g. $x cannot be used before $w has been used\n",
        "    Usable variables include all previously used variables and the next in alphabetical order\n",
        "    :return: set of possible next variables to use\n",
        "    \"\"\"\n",
        "    num_vars_introduced = len(self.variables_introduced())\n",
        "    return set(VARS[:min(self.parse_constraints.max_vars, num_vars_introduced + 1)])\n",
        "\n",
        "  def to_logical_form(self) -> Tuple:\n",
        "    \"\"\"\n",
        "    Convert a completed parser state to a logical form\n",
        "    :return: logical form as a tuple\n",
        "    \"\"\"\n",
        "    assert self.is_complete\n",
        "\n",
        "    # list of tuples\n",
        "    clause_literals = self.completed_literal_stack.tolist()\n",
        "\n",
        "    assert len(clause_literals) > 0\n",
        "    logical_form = (AND,) + tuple(clause_literals)\n",
        "\n",
        "    assert self.lambda_var is not None\n",
        "    vars_to_quantify = tuple(sorted(v for v in self.variables_introduced() if v != self.lambda_var))\n",
        "    if len(vars_to_quantify) > 0:\n",
        "      logical_form = (EXISTS,) + vars_to_quantify + (logical_form,)\n",
        "    logical_form = (LAMBDA, self.lambda_var, logical_form)\n",
        "\n",
        "    return logical_form\n",
        "\n",
        "  @staticmethod\n",
        "  def from_logical_form(logical_form: Tuple, parse_constraints: ParseConstraints=None):\n",
        "    \"\"\"\n",
        "    :param logical_form:\n",
        "    :param parse_constraints: used to check the logical form\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # depth-first post-fix traversal, where actions are node labels, except we\n",
        "    # (1) collapse (lambda $LAMBDA_VAR (exists $VAR1 $VAR2 to COMPLETE_LAMBDA_$LAMBDA_VAR\n",
        "    # (2) do not produce an action for the AND label\n",
        "    # (3) add a DONE_ACTION action at the end (root)\n",
        "    if parse_constraints is None:\n",
        "      parse_constraints = ParseConstraints(\n",
        "        possible_predicates=None, max_vars=len(VARS), no_repeated_literals=False,\n",
        "        max_literals=MAX_LITERALS\n",
        "      )\n",
        "\n",
        "    def _traverse(node: Union[tuple, str], state: ParserState):\n",
        "      if isinstance(node, tuple):\n",
        "        label = node[0]\n",
        "        if label == LAMBDA:\n",
        "          lambda_var = node[1]\n",
        "          assert lambda_var in VARS\n",
        "          children = node[2:]\n",
        "          assert children\n",
        "        elif label == EXISTS:\n",
        "          children = node[1:]\n",
        "          while children[0] in VARS:\n",
        "            children = children[1:]\n",
        "          assert children\n",
        "        else:\n",
        "          children = node[1:]\n",
        "      else:  # is a leaf; should be a variable\n",
        "        label = node\n",
        "        children = []\n",
        "      for child in children:\n",
        "        state = _traverse(child, state)\n",
        "      if label == LAMBDA:\n",
        "        state = state.take_action(VARS_TO_COMPLETE_LAMBDA_ACTIONS[lambda_var])\n",
        "      elif label != AND and label != EXISTS:\n",
        "        state = state.take_action(label)\n",
        "      return state\n",
        "\n",
        "    state = ParserState.initial_state(parse_constraints)\n",
        "    state = _traverse(logical_form, state)\n",
        "    state = state.take_action(DONE_ACTION)\n",
        "    assert state.is_complete\n",
        "    return state\n",
        "\n",
        "  def actions(self) -> List[str]:\n",
        "    return self.past_actions.tolist()\n",
        "\n",
        "  def valid_actions(self) -> List[str]:\n",
        "    # actions that can be taken if we've finished building the lambda expression\n",
        "    if self.is_complete:\n",
        "      return []\n",
        "    if self.has_lambda:\n",
        "      return [DONE_ACTION]\n",
        "\n",
        "    # otherwise, deal with each possible action type in turn\n",
        "    valid_actions = []\n",
        "\n",
        "    # complete the lambda expression\n",
        "    if self.variable_argument_stack.size == 0 and self.completed_literal_stack.size > 0:\n",
        "      for var in self.variables_introduced():\n",
        "        valid_actions.append(VARS_TO_COMPLETE_LAMBDA_ACTIONS[var])\n",
        "\n",
        "    # introduce a predicate, with arguments from argument_stack\n",
        "    if self.variable_argument_stack.size > 0:\n",
        "      possible_predicates = self.parse_constraints.possible_predicates\n",
        "      if possible_predicates is None:\n",
        "        possible_predicates = CATS + RELS + list(ENTITY_ACTIONS_TO_ENTITIES.keys())\n",
        "      possible_predicates = list(sorted(possible_predicates))\n",
        "\n",
        "      for predicate in possible_predicates:\n",
        "        if ParserState.PREDICATE_ARITIES[predicate] == self.variable_argument_stack.size:\n",
        "          valid_actions.append(predicate)\n",
        "\n",
        "    # add a variable to the argument stack\n",
        "    if self.variable_argument_stack.size < ParserState.MAX_ARITY and (\n",
        "        # only start a new literal if we're strictly less than the maximum\n",
        "        (self.variable_argument_stack.size == 0 and self.completed_literal_stack.size < self.parse_constraints.max_literals) or\n",
        "        # only continue an existing literal if we're no greater than the maximum\n",
        "        (self.variable_argument_stack.size > 0 and self.completed_literal_stack.size <= self.parse_constraints.max_literals)\n",
        "    ):\n",
        "      valid_actions.extend(self.variables_usable())\n",
        "\n",
        "    return valid_actions\n",
        "\n",
        "  def __repr__(self):\n",
        "    return \"\"\"ParserState(\n",
        "variable_argument_stack={},\n",
        "completed_literal_stack={},\n",
        "has_lambda={},\n",
        "lambda_var={},\n",
        "is_complete={},\n",
        "all_vars_introduced={},\n",
        "past_actions={},\n",
        "parse_constraints={},\n",
        ")\"\"\".format(*self)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEehJ9krW-1R"
      },
      "source": [
        "With a correct implementation of `take_action`, the following three test cells should run without errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RC_TXaC1A2bc"
      },
      "source": [
        "permissive_parse_constraints = ParseConstraints(\n",
        "    possible_predicates=PREDICATES, \n",
        "    max_vars=len(VARS),\n",
        "    no_repeated_literals=False, \n",
        "    max_literals=MAX_LITERALS,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLpbx4hYVGHF"
      },
      "source": [
        "blank_parser_state = ParserState.initial_state(permissive_parse_constraints)\n",
        "parser_state = blank_parser_state.take_action('$w')\n",
        "assert parser_state.variable_argument_stack.tolist() == ['$w']\n",
        "parser_state = parser_state.take_action('$x')\n",
        "assert parser_state.variable_argument_stack.tolist() == ['$w', '$x']\n",
        "parser_state = parser_state.take_action('in-rel')\n",
        "assert parser_state.variable_argument_stack.tolist() == []\n",
        "assert parser_state.completed_literal_stack.tolist() == [('in-rel', '$w', '$x')]\n",
        "assert parser_state.actions() == ['$w', '$x', 'in-rel']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wliw4WGNUNjp"
      },
      "source": [
        "for state in STATES:\n",
        "  world = geoqa.geo.read_world(state)\n",
        "  for question, answer, logical_form in zip(*geoqa.geo.read_data(state)):\n",
        "    if logical_form is None:\n",
        "      continue\n",
        "    parser_state = ParserState.from_logical_form(logical_form, permissive_parse_constraints)\n",
        "    round_trip_logical_form = parser_state.to_logical_form()\n",
        "    assert logical_form == round_trip_logical_form, \"\\n{} !=\\n{}\".format(logical_form, round_trip_logical_form)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9LIMS-ED_13"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvyJagVtC4Wm"
      },
      "source": [
        "Next we'll define a dataset class. Since this dataset is small, and to allow evaluating on databases with entities we didn't see in training, we'll use pre-trained embeddings to represent both words and predicates in the databases. We've downloaded and pre-filtered [fasttext](https://fasttext.cc/) word embeddings trained on English Wikipedia and news, and we'll use the first 50 dimensions to keep our models small."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ef0TLJ_6ES1G"
      },
      "source": [
        "EMBEDDING_DIM = 50\n",
        "word_to_embedding = geoqa.utils.get_word_vectors(\n",
        "    'vecs/wiki-news-300d-1M-subword-filtered.vec', max_dim=EMBEDDING_DIM\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWU9puj1EdOO"
      },
      "source": [
        "word_to_embedding['virginia']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFLnot81a1wb"
      },
      "source": [
        "We've defined a [`GeoDataset`](https://github.com/dpfried/geoqa-release/blob/master/geoqa/dataset.py#L16) class for you in the support code to preprocess the data (don't worry about reading the code though; we'll demonstrate it below). This class tokenizes each question, and associated with each token a list of predicates that could possibly be used in the logical form, based on string matching predicate names against the word. To constrain the search space over possible logical forms, we will only allow predicates for a given question from the union of all its tokens' predicates. That we can do this at all is a somewhat unique feature of this dataset; more complex semantic parsing problems also require learning more of the lexicon as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMo7OM7tYM6g"
      },
      "source": [
        "sample_dataset = GeoDataset(state_names=['wv', 'va'], word_to_embedding=word_to_embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96u2wHRuYQoM"
      },
      "source": [
        "def print_instance(instance, print_tokens=False):\n",
        "  for key in ['question', 'answer', 'logical_form', 'world', 'possible_predicates', 'denotation']:\n",
        "    if key == 'logical_form':\n",
        "      rep = logical_form_to_str(instance[key])\n",
        "    else:\n",
        "      rep = instance[key]\n",
        "    print('{:<19}: {}'.format(key, rep))\n",
        "  if print_tokens:\n",
        "    print()\n",
        "    print('{:<10}: {}'.format('word', 'predicates'))\n",
        "    print('='*23)\n",
        "    for word, predicates, embedded_predicates in zip(\n",
        "      instance['words'], \n",
        "      instance['predicates_at_each_word_position'], \n",
        "      instance['embedded_predicates_at_each_word_position']\n",
        "    ):\n",
        "      print('{:<10}: {}'.format(word, ', '.join(predicates)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mg8r1LCvYsKj"
      },
      "source": [
        "sample_instance = sample_dataset[30]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAQ_HnvGcdCS"
      },
      "source": [
        "print_instance(sample_instance, print_tokens=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nue9mxkodoMi"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7MlSw_cM7dn"
      },
      "source": [
        "In this section we'll define a neural sequence-to-sequence model that encodes a question and outputs the actions needed to produce its logical form.\n",
        "\n",
        "We will use a `_ModelState` class to wrap a `ParserState` and also contain a model's hidden state (for the sequence-to-sequence decoder) and the log probabilities of the next possible actions that can be taken from the parser state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpogB4kWdplD"
      },
      "source": [
        "_ModelState = namedtuple('_ModelState', (\n",
        "  'parser_state',\n",
        "  # ParserState: the current parser state\n",
        "  'hidden_state',\n",
        "  # a tuple (h, c) of pytorch tensors\n",
        "  'action_log_probs'\n",
        "  # Dict[str, tensor]: a mapping from next possible actions (parser_state.valid_actions) to\n",
        "  # a pytorch scalar giving the log probability of that action\n",
        "))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pkgqpXqMmRB"
      },
      "source": [
        "The `ModelState` class will inherit from `_ModelState` and add a `take_action` method which creates a new `ModelState`, updated using a forward pass of the model and the `take_action` method of the `ParserState`.\n",
        "\n",
        "Here's a DummyModel which always produces a uniform probability distribution over all available actions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmZDJftxYTs_"
      },
      "source": [
        "class DummyModel(nn.Module):\n",
        "  # model that produces scores of zero always; use to explore logical forms\n",
        "  def __init__(self, max_vars=len(VARS), no_repeated_literals=False, max_literals=MAX_LITERALS):\n",
        "    super(DummyModel, self).__init__()\n",
        "    self.max_vars = max_vars\n",
        "    self.max_literals = max_literals\n",
        "    self.no_repeated_literals = no_repeated_literals\n",
        "\n",
        "  def forward(self, actions_to_score):\n",
        "    action_log_probs = {\n",
        "      # a scalar tensor (dimension 0)\n",
        "      action: torch.tensor(1.0/len(actions_to_score)).log()\n",
        "      for action in actions_to_score\n",
        "    }\n",
        "    hidden_state = None\n",
        "    return action_log_probs, hidden_state\n",
        "\n",
        "  def initialize_model_state(self, instance):\n",
        "    parse_constraints = ParseConstraints(\n",
        "      possible_predicates=instance['possible_predicates'],\n",
        "      max_vars=self.max_vars,\n",
        "      no_repeated_literals=self.no_repeated_literals,\n",
        "      max_literals=self.max_literals,\n",
        "    )\n",
        "    # define self as a variable so that we can reference it inside the DummyModelState class without using self;\n",
        "    model = self\n",
        "\n",
        "    class DummyModelState(_ModelState):\n",
        "      @staticmethod\n",
        "      def _create_from_parser_state(parser_state):\n",
        "        possible_actions = parser_state.valid_actions()\n",
        "        action_log_probs, hidden_state = model.forward(possible_actions)\n",
        "        return DummyModelState(parser_state, hidden_state, action_log_probs)\n",
        "\n",
        "      def take_action(self, action: str):\n",
        "        new_parser_state = self.parser_state.take_action(action)\n",
        "        return DummyModelState._create_from_parser_state(new_parser_state)\n",
        "\n",
        "    parser_state = ParserState.initial_state(parse_constraints)\n",
        "    return DummyModelState._create_from_parser_state(parser_state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mikRkcXLO9Z-"
      },
      "source": [
        "We can use this DummyModel to explore the space of possible (constrained) logical forms for a given instance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vk4g_VTwbUBC"
      },
      "source": [
        "def greedy_search(model, instance):\n",
        "  state = model.initialize_model_state(instance)\n",
        "  log_probs = []\n",
        "  # when we disallow repeated literals, we can get to parser states with no available actions\n",
        "  # so we need to check to ensure there are scored items in `state.action_log_probs`\n",
        "  while state.action_log_probs.items() and not state.parser_state.is_complete:\n",
        "    action, log_prob = max(state.action_log_probs.items(), key=lambda tuple: tuple[1].item())\n",
        "    log_probs.append(log_prob)\n",
        "    state = state.take_action(action)\n",
        "  return state, sum(log_probs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CesXN_O1PG_R"
      },
      "source": [
        "print_instance(sample_instance, print_tokens=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-p-B4lRpPCuI"
      },
      "source": [
        "model_state, total_log_prob = greedy_search(\n",
        "    DummyModel(max_vars=4, no_repeated_literals=False),\n",
        "    sample_instance\n",
        "    )\n",
        "print(logical_form_to_str(model_state.parser_state.to_logical_form()))\n",
        "print(total_log_prob)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wE_ndLLPpeH"
      },
      "source": [
        "Now, we've defined an (unbatched) beam search for you in the two cells below. Only a small number of actions will be allowed to be taken at any point in time by our transition system, so we've chosen to use dictionaries to map actions to scores:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6Gv1Snfq0SW"
      },
      "source": [
        "def k_best(score_dict, k):\n",
        "  k = min(k, len(score_dict))\n",
        "  if k == 0:\n",
        "    return {}\n",
        "  actions, logits = zip(*score_dict.items())\n",
        "  logits = torch.stack(logits, dim=-1).flatten()\n",
        "  chosen_scores, indices = logits.topk(k, dim=-1)\n",
        "  indices = indices\n",
        "  return {\n",
        "    actions[index.item()]: score\n",
        "    for score, index in zip(chosen_scores, indices)\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtciQnNlq4z4"
      },
      "source": [
        "def beam_search(model, instance, beam_size):\n",
        "  state = model.initialize_model_state(instance)\n",
        "  completed = []\n",
        "  # each beam item will be a tuple (state, cumulative log probs)\n",
        "  beam = [(state, torch.tensor(0.0))]\n",
        "  while beam and len(completed) < beam_size:\n",
        "    successors = []\n",
        "    for state, log_prob in beam:\n",
        "      scored_successors = k_best(state.action_log_probs, beam_size)\n",
        "      for action, action_log_prob in scored_successors.items():\n",
        "        successor_log_prob = log_prob + action_log_prob\n",
        "        successors.append((successor_log_prob, state, action))\n",
        "    # when we disallow repeated literals, we can get to parser states with no available actions\n",
        "    # so we need to check to see if there are successors available\n",
        "    if not successors:\n",
        "      break\n",
        "    log_probs = torch.stack([log_prob for log_prob, _, _ in successors], dim=-1).flatten()\n",
        "    if beam_size is None:\n",
        "      indices = torch.arange(len(successors))\n",
        "    else:\n",
        "      _, indices = log_probs.topk(min(len(successors), beam_size), dim=-1)\n",
        "    new_beam = []\n",
        "    for ix in indices:\n",
        "      log_prob, prev_state, action = successors[ix.item()]\n",
        "      state = prev_state.take_action(action)\n",
        "      if state.parser_state.is_complete:\n",
        "        completed.append((state, log_prob))\n",
        "      else:\n",
        "        new_beam.append((state, log_prob))\n",
        "    beam = new_beam\n",
        "\n",
        "  completed = sorted(completed, key=lambda t: t[1].item(), reverse=True)\n",
        "  completed = completed[:beam_size]\n",
        "  return completed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHfwDe28Q2Jv"
      },
      "source": [
        "def sample_search_model(model, beam_size):\n",
        "  \"\"\"\n",
        "  A convenience method for printing out beam search outputs\n",
        "  \"\"\"\n",
        "  for model_state, total_log_prob in beam_search(model, sample_instance, beam_size):\n",
        "      # with the transition system we've defined,\n",
        "      # it's possible to get into a parser state with no available actions\n",
        "      # so we need to check to ensure the parser state is complete before\n",
        "      # converting it to a logical form\n",
        "    if model_state.parser_state.is_complete:\n",
        "      logical_form = model_state.parser_state.to_logical_form()\n",
        "      print(\"{:.2f} {}\".format(total_log_prob.item(), logical_form_to_str(logical_form)))\n",
        "    else:\n",
        "      print(\"incomplete logical form\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2ShzVMuP0ia"
      },
      "source": [
        "print_instance(sample_instance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgGWNRJIP0iq"
      },
      "source": [
        "sample_search_model(DummyModel(max_vars=1, no_repeated_literals=False), beam_size=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJDNkbrLv4OP"
      },
      "source": [
        "You may notice that some literals are repeated within a single logical form, e.g. `... (border-rel $w $w) (border-rel $w $w) ...` (due to randomness, you may not see this exact example in the candidates above). Since these identical literals occur inside a conjunction, they're logically redundant. Because of this, we can prevent duplicate literals to reduce the search space, without affecting the expressiveness of the logical forms.\n",
        "\n",
        "Modify the `valid_actions` function below to prevent duplicate literals, if `self.parse_constraints.no_repeated_literals` is `True`. (We've copied this function from `ParserState.valid_actions`, and will monkey-patch `ParserState` with it, so that you don't have to scroll back and forth.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7oeosbqwiYh"
      },
      "source": [
        "def valid_actions(self: ParserState) -> List[str]:\n",
        "    # actions that can be taken if we've finished building the lambda expression\n",
        "    if self.is_complete:\n",
        "      return []\n",
        "    if self.has_lambda:\n",
        "      return [DONE_ACTION]\n",
        "\n",
        "    # otherwise, deal with each possible action type in turn\n",
        "    valid_actions = []\n",
        "\n",
        "    # complete the lambda expression\n",
        "    if self.variable_argument_stack.size == 0 and self.completed_literal_stack.size > 0:\n",
        "      for var in self.variables_introduced():\n",
        "        valid_actions.append(VARS_TO_COMPLETE_LAMBDA_ACTIONS[var])\n",
        "\n",
        "    # introduce a predicate, with arguments from argument_stack\n",
        "    if self.variable_argument_stack.size > 0:\n",
        "      possible_predicates = self.parse_constraints.possible_predicates\n",
        "      if possible_predicates is None:\n",
        "        possible_predicates = CATS + RELS + list(ENTITY_ACTIONS_TO_ENTITIES.keys())\n",
        "      possible_predicates = list(sorted(possible_predicates))\n",
        "\n",
        "      for predicate in possible_predicates:\n",
        "        if ParserState.PREDICATE_ARITIES[predicate] == self.variable_argument_stack.size:\n",
        "          if self.parse_constraints.no_repeated_literals:\n",
        "            # TODO: modify this method so that no repeated literals are allowed in the logical form\n",
        "            \"\"\"YOUR CODE HERE\"\"\"\n",
        "\n",
        "            ...\n",
        "\n",
        "          valid_actions.append(predicate)\n",
        "\n",
        "    # add a variable to the argument stack\n",
        "    if self.variable_argument_stack.size < ParserState.MAX_ARITY and (\n",
        "        # only start a new literal if we're strictly less than the maximum\n",
        "        (self.variable_argument_stack.size == 0 and self.completed_literal_stack.size < self.parse_constraints.max_literals) or\n",
        "        # only continue an existing literal if we're no greater than the maximum\n",
        "        (self.variable_argument_stack.size > 0 and self.completed_literal_stack.size <= self.parse_constraints.max_literals)\n",
        "    ):\n",
        "      valid_actions.extend(self.variables_usable())\n",
        "\n",
        "    return valid_actions\n",
        "\n",
        "ParserState.valid_actions = valid_actions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwSQ6pIgxdiU"
      },
      "source": [
        "With a correct implementation, you should now see that literals are not repeated:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzwuZQPyv31I"
      },
      "source": [
        "sample_search_model(DummyModel(max_vars=1, no_repeated_literals=True), beam_size=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeGC1PWcI7Ea"
      },
      "source": [
        "The test cell below, which checks to ensure that there are no repeated literals, should pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nd4as9-8e8pt"
      },
      "source": [
        "for model_state, score in beam_search(DummyModel(max_vars=1, no_repeated_literals=True), sample_instance, 1000):\n",
        "  completed_literals = model_state.parser_state.completed_literal_stack.tolist()\n",
        "  # check for duplicate literals\n",
        "  assert len(completed_literals) == len(set(completed_literals))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtjbJKN0xjB8"
      },
      "source": [
        "And the test from before of logical form -> parser state -> logical form (now using `no_repeated_literals=True`) should still pass:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hYyMcn_e4Jz"
      },
      "source": [
        "no_repeats_parse_options = ParseConstraints(\n",
        "    possible_predicates=PREDICATES, \n",
        "    max_vars=len(VARS), \n",
        "    no_repeated_literals=True, \n",
        "    max_literals=MAX_LITERALS\n",
        ")\n",
        "for logical_form in logical_forms:\n",
        "  if logical_form is None:\n",
        "    continue\n",
        "  parser_state = ParserState.from_logical_form(logical_form, no_repeats_parse_options)\n",
        "  round_trip_logical_form = parser_state.to_logical_form()\n",
        "  assert logical_form == round_trip_logical_form, \"{} != {}\".format(logical_form, round_trip_logical_form)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGKvYAWxSfMj"
      },
      "source": [
        "We'll now evaluate this `DummyModel` as a very simple baseline, using [geoqa.evaluation.evaluate_predictions](https://github.com/dpfried/geoqa-release/blob/master/geoqa/evaluation.py), in the two cells below. You might be surprised at the accuracy; a few structural and lexical constraints can go a long way on this dataset, and it's also sometimes possible to get the correct denotation through an incorrect logical form (this will complicate things later on, when we learn from denotations alone). However, examining some instances will show that this baseline model has very poor performance on complex examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhK6TbZAOd12"
      },
      "source": [
        "Note: some of the questions are yes/no questions (e.g. _is virginia east of west virginia_). If the denotation for a question is empty (`set()`), it should be interpreted as a \"no\" answer; if the denotation is non-empty (`{'virginia'}`) it should be interpreted as a yes. A different version of this dataset, introduced by [Andreas et al. 2016](https://arxiv.org/abs/1601.01705), adds an `any` operation to the logical forms for yes/no questions, which maps empty denotations to \"no\" and non-empty to \"yes\", but this makes training from denotations (which we'll do later on) more difficult."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RceZcr5D4Fzn"
      },
      "source": [
        "def make_greedy_prediction_function(model):\n",
        "  # helper function to make a prediction function for evaluate_predictions for a model\n",
        "  def prediction_function(instance):\n",
        "    state, _ = greedy_search(model, instance)\n",
        "    if state.parser_state.is_complete:\n",
        "      return state.parser_state.to_logical_form()\n",
        "    else:\n",
        "      None\n",
        "  return prediction_function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYhCXEm9SmxO"
      },
      "source": [
        "dummy_model = DummyModel(max_vars=4, no_repeated_literals=True)\n",
        "evaluate_predictions(\n",
        "  GeoDataset(DEV_STATES, word_to_embedding), 'dev', execute,\n",
        "  prediction_function=make_greedy_prediction_function(dummy_model),\n",
        "  display_predictions_frequency=5\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV7-bqimfHLs"
      },
      "source": [
        "Now we'll implement a neural model to train on the dataset. We've defined the encoder for you in the cell below, following one possible implementation for the encoder from project 2. The dataset is small enough that batching and using a GPU are unnecessary, so we will leave them out for simplicity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4EhpM-nQaXa"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, hidden_dim, word_vector_dim, dropout):\n",
        "    # word_vectors: vocab_size x dim\n",
        "    super(Encoder, self).__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.word_vector_dim = word_vector_dim\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.lstm = nn.LSTM(self.word_vector_dim, hidden_dim,\n",
        "                        bidirectional=True, batch_first=True)\n",
        "\n",
        "  def forward(self, embedded_sentence):\n",
        "    # embedded_sentence: T x word_vector_dim\n",
        "    lstm_out, (h_n, c_n) = self.lstm(embedded_sentence.unsqueeze(0))\n",
        "\n",
        "    # eliminate batch dimension\n",
        "    h_n = h_n.squeeze(0)\n",
        "    c_n = c_n.squeeze(0)\n",
        "\n",
        "    # n_layers x directions x T\n",
        "    h_n = h_n.view(1, 2, -1).mean(1)\n",
        "    c_n = c_n.view(1, 2, -1).mean(1)\n",
        "\n",
        "    # eliminate batch dimension\n",
        "    lstm_out = lstm_out.squeeze(0)\n",
        "    return self.dropout(lstm_out), (h_n, c_n)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhgBh1oQNeaw"
      },
      "source": [
        "Before building the decoder, we'll define some helper functions to work with probability distributions over actions in log space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqY4QGRy4KDe"
      },
      "source": [
        "# helper functions to deal with probabilities in log space\n",
        "def sum_log_probs(log_probs):\n",
        "  # log_probs: List[tensor]\n",
        "  # compute \\log \\sum_x \\exp x\n",
        "  return torch.logsumexp(torch.stack(\n",
        "    list(log_probs), dim=-1\n",
        "  ), -1)\n",
        "\n",
        "def renormalize_log_prob_dict(log_prob_dict):\n",
        "  # log_prob_dict: Dict[str, tensor]\n",
        "  # rescale the log probs so that when exponentiated, they sum to 1\n",
        "  if not log_prob_dict:\n",
        "    return log_prob_dict\n",
        "  Z = sum_log_probs(log_prob_dict.values())\n",
        "  return {k: v - Z for k, v in log_prob_dict.items()}\n",
        "\n",
        "def filter_log_prob_dict(log_prob_dict, valid_actions):\n",
        "  return {\n",
        "    k: v for k, v in log_prob_dict.items()\n",
        "    if k in valid_actions\n",
        "  }\n",
        "\n",
        "def is_normalized(log_prob_dict):\n",
        "  # log_prob_dict: Dict[str, tensor]\n",
        "  # check that this dictionary is normalized in log space\n",
        "  # e.g. as returned by renormalize_log_prob_dict\n",
        "  if not log_prob_dict:\n",
        "    return True\n",
        "  return torch.allclose(\n",
        "    sum_log_probs(log_prob_dict.values()).exp(),\n",
        "    torch.tensor(1.0),\n",
        "    atol=1e-3\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojwfaXywfRLm"
      },
      "source": [
        "You'll now define part of the Decoder. You'll implement an attention-based pointer-generator model (based roughly on section 3.2 of [Jia and Liang 2016](https://arxiv.org/pdf/1606.03622.pdf) and section 2.2 of [See et al. 2017](https://arxiv.org/pdf/1704.04368.pdf)), which is useful for sequence-to-sequence tasks where parts of the output are copied from the input (in our case, predicates will be copied from the tokens they are associated with). \n",
        "\n",
        "Actions can be produced either through a `generate` action (given by predicting them from a standard projection layer, similar to the vocab prediction in the previous assignments) or a `copy` action: \"pointing\" to them in the set of predicates associated with each token in the question, using an attention mechanism. \n",
        "\n",
        "An action is produced by marginalizing over a binary choice to `copy` or `generate`, with $p(copy) = 1 - p(generate)$:\n",
        "\n",
        "$p(action) = p(action | generate) * p(generate) + p(action | copy) * p(copy)$\n",
        "\n",
        "`STRUCTURAL_ACTIONS` can only be produced by generating. `DATABASE_SPECIFIC_PREDICATE_ACTIONS` can only be produced by copying (since all of these will be unseen when evaluating on a new database). `CROSS_DATABASE_PREDICATE_ACTIONS` can be produced either by generating or copying.\n",
        "\n",
        "\n",
        "You'll implement this in the `forward()` method of `Decoder`, computing $p(action)$ for each action in `actions_to_score` by combining the probabilities as above. Below are details about the generation mechanism (which we've implemented) and copy mechanism (which you'll implement).\n",
        "\n",
        "__Generation mechanism__\n",
        "\n",
        "$p(action | generate)$ is produced using a standard projection layer similar to the vocab prediction in the previous assignments, assigning scores to actions in `STRUCTURAL_ACTIONS` + `CROSS_DATABASE_PREDICATE_ACTIONS`. We've implemented this for you, in `_generate_log_probs()`.\n",
        "\n",
        "__Copy mechanism__\n",
        "\n",
        "Actions for predicates that are associated with words in the sentence can be produced by \"pointing\" to them with an attention mechanism. We will reuse the decoder's attention mechanism, masked and renormalized to only be over those tokens that have associated predicates, to give a distribution $p(position | copy)$. A given predicate can appear with multiple tokens, so marginalize over the positions in the sentence:\n",
        "\n",
        "$p(action | copy) = \\sum_{position} p(action | position, copy) p(position | copy)$\n",
        "\n",
        "A given position can have zero, one, or multiple predicates associated with it (e.g. the token \"ocean\" can have both the `ocean` or the `kb-atlantic_ocean` predicates associated with it); produce a distribution over the predicates at each position using a softmax of the dot product between a `predicate_key` vector (a projection of the decoder state, which we've computed for you) and the pretrained embedding for the action's predicate:\n",
        "\n",
        "$p(action | position, copy) = softmax(\\texttt{predicate_embedding}^\\top \\texttt{predicate_key})$\n",
        "\n",
        "You'll implement this in the `.copy_log_probs()` method in `Decoder` below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUuvdfF1OTz4"
      },
      "source": [
        "Fill in the `._copy_log_probs()` and `.forward()` methods in the `Decoder` class below.\n",
        "\n",
        "__Implementation tip__: Do probability calculations in log space: `p*q` can be computed using `+` when values are represented as log probabilities. In the same way, the `torch.logsumexp` function computes the sum `p+q` when `p` and `q` (and the returned value) are represented as log probabilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feAB09qGQv_U"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  START_ACTION = \"<START>\"\n",
        "  DATABASE_SPECIFIC_ACTION = \"<KB_ENTITY>\"\n",
        "\n",
        "  def __init__(self, hidden_dim, action_embedding_dim, input_action_vec_dim, dropout):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.action_embedding_dim = action_embedding_dim\n",
        "    self.input_action_vec_dim = input_action_vec_dim\n",
        "\n",
        "    self.action_index = Index()\n",
        "    generable_indices = []\n",
        "    for action in STRUCTURAL_ACTIONS:\n",
        "      generable_indices.append(self.action_index.index(action))\n",
        "    for action in CROSS_DATABASE_PREDICATE_ACTIONS:\n",
        "      generable_indices.append(self.action_index.index(action))\n",
        "    self.generable_indices = generable_indices\n",
        "    self.START_INDEX = self.action_index.index(Decoder.START_ACTION)\n",
        "    self.DATABASE_SPECIFIC_ACTION_INDEX = self.action_index.index(Decoder.DATABASE_SPECIFIC_ACTION)\n",
        "    self.action_index.frozen = True\n",
        "\n",
        "    self.action_embeddings = nn.Embedding(self.action_index.size(), self.action_embedding_dim)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.lstm_cell = nn.LSTMCell(self.action_embedding_dim, self.hidden_dim)\n",
        "\n",
        "    self.action_scoring_layer = nn.Linear(hidden_dim, self.input_action_vec_dim)\n",
        "\n",
        "    self.encoder_projection = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "    self.copy_or_generate_layer = nn.Linear(hidden_dim, 2)\n",
        "    self.generate_layer = nn.Linear(hidden_dim, len(generable_indices))\n",
        "\n",
        "  def _pre_prediction(self, encoder_output, last_action, last_hidden):\n",
        "    if last_action in self.action_index:\n",
        "      action_index = self.action_index.index(last_action)\n",
        "    elif last_action is None:\n",
        "      action_index = self.START_INDEX\n",
        "    else:\n",
        "      action_index = self.DATABASE_SPECIFIC_ACTION_INDEX\n",
        "\n",
        "    embedded_action = self.action_embeddings(torch.LongTensor([action_index]))\n",
        "    h, c = self.lstm_cell(embedded_action, last_hidden)\n",
        "    hidden_state = (h, c)\n",
        "    decoder_out = h.squeeze(0)\n",
        "    decoder_out = self.dropout(decoder_out)\n",
        "\n",
        "    encoder_output_projected = self.encoder_projection(encoder_output)\n",
        "\n",
        "    attention_logits = torch.einsum(\"th,h->t\", [encoder_output_projected, decoder_out])\n",
        "    attention_dist = torch.softmax(attention_logits, dim=-1)\n",
        "    pooled_inputs = torch.einsum(\"th,t->h\", [encoder_output_projected, attention_dist])\n",
        "\n",
        "    output_with_attention = decoder_out + pooled_inputs\n",
        "\n",
        "    return hidden_state, output_with_attention, attention_logits\n",
        "\n",
        "  def _position_attention(self, predicates_at_each_word_position, attention_logits):\n",
        "    positions_without_actions = [ix for ix, l in enumerate(predicates_at_each_word_position) if not l]\n",
        "    position_attention_logits = attention_logits.clone()\n",
        "    position_attention_logits[positions_without_actions] = BIG_NEG\n",
        "    if len(positions_without_actions) == len(predicates_at_each_word_position):\n",
        "      raise ValueError('no valid positions with predicates to copy to')\n",
        "    return torch.log_softmax(position_attention_logits, dim=-1)\n",
        "\n",
        "  def _generate_log_probs(self, actions_to_score, output_with_attention):\n",
        "    # return log p(action | generate) for actions in actions_to_score,\n",
        "    # as a dictionary mapping actions to torch scalars\n",
        "    # log probabilities will be normalized over all generable actions; *NOT* over just actions_to_score\n",
        "    # (renormalization will happen at the end of forward(), after all marginalizations are complete)\n",
        "    logits = self.generate_layer(output_with_attention)\n",
        "    log_probs = torch.log_softmax(logits, dim=-1)\n",
        "    return {\n",
        "      action: log_probs[self.action_index.index(action)]\n",
        "      for action in actions_to_score\n",
        "      if action in self.action_index\n",
        "    }\n",
        "\n",
        "  def _copy_log_probs(self,\n",
        "                         predicate_key,\n",
        "                         predicates_at_each_word_position,\n",
        "                         embedded_predicates_at_each_word_position,\n",
        "                         position_attention_log_probs,\n",
        "                         copyable_actions):\n",
        "    \"\"\"\n",
        "\n",
        "    :param predicate_key:\n",
        "      tensor of size (embedded_predicate_dim,)\n",
        "    :param predicates_at_each_word_position:\n",
        "      List[List[str]], containing the list of predicates associated with each position in the sentence\n",
        "    :param embedded_predicates_at_each_word_position:\n",
        "      List[tensor], containing the predicate embeddings associated with each position in the sentence.\n",
        "      embedded_predicates_at_each_word_position[i].size() == len(predicates_at_each_word_position[i]), embedding_dim\n",
        "    :param position_attention_log_probs:\n",
        "      tensor of size (len(sentence)), containing log p(position | copy)\n",
        "    :param copyable_actions:\n",
        "      Set[str], containing all predicates that can be copied to in this sentence\n",
        "    :return:\n",
        "      log p(action | copy) for actions in actions_to_score,\n",
        "      as a dictionary mapping actions to torch scalars\n",
        "      this dictionary should be normalized (in log space), over all copyable actions\n",
        "      Implementation tip: you shouldn't have to normalize this yourself, it should happen\n",
        "      by construction\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"YOUR CODE HERE\"\"\"\n",
        "\n",
        "    ...\n",
        "\n",
        "  def forward(self,\n",
        "              encoder_output,\n",
        "              predicates_at_each_word_position,  # [[action, vector]]\n",
        "              embedded_predicates_at_each_word_position,\n",
        "              last_action,\n",
        "              last_hidden_state,\n",
        "              actions_to_score):\n",
        "    \"\"\"\n",
        "    :return:\n",
        "      * action_log_probs: a Dict[str, tensor], mapping each action in actions_to_score to a scalar tensor, giving the action's log probability\n",
        "      * hidden_state: (h, c), decoder's updated hidden state after taking last_action\n",
        "    \"\"\"\n",
        "    T, hidden_dim = encoder_output.size()\n",
        "\n",
        "    # hidden_state: (h, c), each a tensor of size (hidden_dim,)\n",
        "    # output_with_attention: tensor of size (hidden_dim,)\n",
        "    # attention_logits: tensor of size (T,)\n",
        "    hidden_state, output_with_attention, attention_logits = self._pre_prediction(\n",
        "      encoder_output, last_action, last_hidden_state\n",
        "    )\n",
        "\n",
        "    all_predicates = set([p for ps in predicates_at_each_word_position for p in ps])\n",
        "    #\n",
        "\n",
        "    # copy_lp and generate_lp are each scalar tensors, giving log p(copy) and log p(generate)\n",
        "    copy_lp, generate_lp = torch.log_softmax(self.copy_or_generate_layer(output_with_attention), dim=-1)\n",
        "\n",
        "    # generate_action_log_probs. log p(action | generate), for generable actions in actions_to_score\n",
        "    # a Dict[str, tensor], mapping each action in actions_to_score to a scalar tensor, giving the action's log probability\n",
        "    generate_action_log_probs = self._generate_log_probs(actions_to_score, output_with_attention)\n",
        "\n",
        "    # position_attention_log_probs: tensor of size (T,) giving log p(position | copy)\n",
        "    position_attention_log_probs = self._position_attention(predicates_at_each_word_position, attention_logits)\n",
        "\n",
        "    # compute log p(action | copy) for actions in all_predicates (the copyable actions)\n",
        "    # a Dict[str, tensor], mapping each action in all_predicates to a scalar tensor, giving the action's log probability\n",
        "    copy_action_log_probs = self._copy_log_probs(\n",
        "      self.action_scoring_layer(output_with_attention),\n",
        "      predicates_at_each_word_position,\n",
        "      embedded_predicates_at_each_word_position,\n",
        "      position_attention_log_probs,\n",
        "      all_predicates\n",
        "    )\n",
        "    if copy_action_log_probs:\n",
        "      assert is_normalized(copy_action_log_probs)\n",
        "    # now, filter this down to only those actions we want to score\n",
        "    copy_action_log_probs = filter_log_prob_dict(copy_action_log_probs, actions_to_score)\n",
        "\n",
        "    # compute the probability of each action, p(action) as:\n",
        "    # p(action) = p(action | generate) * p(generate) + p(action | copy) * p(copy)\n",
        "    # p(position | copy) is given by position_attention_log_probs (a tensor)\n",
        "    # p(action | generate) and p(action | copy) are given by generate_action_log_probs and copy_action_log_probs (dictionaries), above\n",
        "\n",
        "    \"\"\"YOUR CODE HERE\"\"\"\n",
        "    \n",
        "    ...\n",
        "\n",
        "    assert all(a in action_log_probs for a in actions_to_score)\n",
        "    # since this contains only a subset of actions, renormalize (in log-space)\n",
        "    filtered = renormalize_log_prob_dict(\n",
        "      filter_log_prob_dict(action_log_probs, valid_actions=actions_to_score)\n",
        "    )\n",
        "    for v in filtered.values():\n",
        "      assert v > -1e8\n",
        "    return filtered, hidden_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnm44g0Aesip"
      },
      "source": [
        "The cell below defines a `Model` class using this `Encoder` and `Decoder`, with the same interface we used in `DummyModel`. We've hardcoded some hyperparameters for size and dropout that we found to work well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnvpPyELQ1IT"
      },
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self, max_vars=len(VARS), no_repeated_literals=True, max_literals=MAX_LITERALS):\n",
        "    super(Model, self).__init__()\n",
        "    self.max_vars = max_vars\n",
        "    self.max_literals = max_literals\n",
        "    self.no_repeated_literals = no_repeated_literals\n",
        "\n",
        "    self.hidden_dim = 50\n",
        "    self.action_embedding_dim = 50\n",
        "    self.dropout = 0.2\n",
        "\n",
        "    self.encoder = Encoder(self.hidden_dim, EMBEDDING_DIM, self.dropout)\n",
        "    self.decoder = Decoder(self.hidden_dim, self.action_embedding_dim, EMBEDDING_DIM, self.dropout)\n",
        "    self.max_vars = max_vars\n",
        "\n",
        "  def initialize_model_state(self, instance):\n",
        "    parse_constraints = ParseConstraints(\n",
        "      possible_predicates=instance['possible_predicates'],\n",
        "      max_vars=self.max_vars,\n",
        "      no_repeated_literals=self.no_repeated_literals,\n",
        "      max_literals=self.max_literals,\n",
        "    )\n",
        "    parser_state = ParserState.initial_state(parse_constraints)\n",
        "\n",
        "    encoder_output, encoder_hidden = self.encoder(instance['embedded_words'])\n",
        "\n",
        "    # define a decoder variable so that we can reference it inside the ModelState class without using self;\n",
        "    decoder = self.decoder\n",
        "\n",
        "    # embed this class so that we don't have to reencode the input every time we take an action\n",
        "    class ModelState(_ModelState):\n",
        "      @staticmethod\n",
        "      def _create_from_hidden_and_action(last_hidden_state, last_action, parser_state: ParserState):\n",
        "        possible_actions = parser_state.valid_actions()\n",
        "        action_log_probs, hidden = decoder(\n",
        "          encoder_output,\n",
        "          instance['predicates_at_each_word_position'],\n",
        "          instance['embedded_predicates_at_each_word_position'],\n",
        "          last_action,\n",
        "          last_hidden_state,\n",
        "          actions_to_score=possible_actions\n",
        "        )\n",
        "        return ModelState(parser_state, hidden, action_log_probs)\n",
        "\n",
        "      def take_action(self, action: str):\n",
        "        parser_state = self.parser_state.take_action(action)\n",
        "        return ModelState._create_from_hidden_and_action(self.hidden_state, action, parser_state)\n",
        "\n",
        "    # the decoder expects None to start the action sequence\n",
        "    last_action = None\n",
        "    return ModelState._create_from_hidden_and_action(encoder_hidden, last_action, parser_state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjB3AKvAdsK4"
      },
      "source": [
        "## Supervised Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLcB77Tw7gdL"
      },
      "source": [
        "We'll first assume a supervised setting, where logical forms are provided for each question and used to directly supervise the model outputs. Later on, we'll relax this assumption and train from questions and denotations (answers) alone, using latent logical forms. In the two cells below, we've provided training code that will use the model and parser states you've defined."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bK74FVyS5VVl"
      },
      "source": [
        "def make_beam_prediction_function(model, beam_size=5):\n",
        "  # helper function to make a prediction function for evaluate_predictions for a model\n",
        "  def prediction_function(instance):\n",
        "    candidates = beam_search(model, instance, beam_size)\n",
        "    if not candidates:\n",
        "      return None\n",
        "    state, _ = candidates[0]\n",
        "    if state.parser_state.is_complete:\n",
        "      return state.parser_state.to_logical_form()\n",
        "    else:\n",
        "      return None\n",
        "  return prediction_function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfFusJ0SrftT"
      },
      "source": [
        "def train(model, train_dataset, dev_dataset, model_file, num_epochs=10, latent_logical_forms=False,\n",
        "          training_beam_size=None, learning_rate=1e-3, display_predictions_frequency=None):\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  stats_by_epoch = {}\n",
        "\n",
        "  train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=1, collate_fn=lambda x: x)\n",
        "\n",
        "  if latent_logical_forms:\n",
        "    assert training_beam_size is not None\n",
        "\n",
        "  if not latent_logical_forms:\n",
        "    loss_function_for_eval = lambda instance: supervised_loss(model, instance).item()\n",
        "  else:\n",
        "    loss_function_for_eval = None\n",
        "\n",
        "  prediction_function = make_beam_prediction_function(model, 5)\n",
        "\n",
        "  best_metric = 0.0\n",
        "  for epoch in tqdm.notebook.trange(num_epochs, desc=\"training\", unit=\"epoch\"):\n",
        "    with tqdm.notebook.tqdm(\n",
        "        train_dataloader,\n",
        "        desc=\"epoch {}\".format(epoch + 1),\n",
        "        unit=\"instance\",\n",
        "        total=len(train_dataloader)\n",
        "    ) as batch_iterator:\n",
        "      model.train()\n",
        "\n",
        "      total_num_correct_lfs = 0\n",
        "      total_num_lfs = 0\n",
        "\n",
        "      total_loss = 0.0\n",
        "\n",
        "      for i, batch in enumerate(batch_iterator, start=1):\n",
        "        assert len(batch) == 1\n",
        "        instance = batch[0]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        if latent_logical_forms:\n",
        "          loss, num_correct_lfs, num_lfs = latent_loss(model, instance, training_beam_size)\n",
        "          total_num_lfs += num_lfs\n",
        "          total_num_correct_lfs += num_correct_lfs\n",
        "          batch_iterator.set_postfix(\n",
        "            train_correct_candidates_per_instance=total_num_correct_lfs/total_num_lfs*training_beam_size,\n",
        "            train_loss=total_loss / i\n",
        "          )\n",
        "        else:\n",
        "          loss = supervised_loss(model, instance)\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "      model.eval()\n",
        "      train_stats = evaluate_predictions(\n",
        "        train_dataset, 'train', execute,\n",
        "        prediction_function=prediction_function,\n",
        "        loss_function=loss_function_for_eval,\n",
        "        display_predictions_frequency=display_predictions_frequency\n",
        "      )\n",
        "      dev_stats = evaluate_predictions(\n",
        "        dev_dataset, 'dev', execute,\n",
        "        prediction_function=prediction_function,\n",
        "        loss_function=loss_function_for_eval,\n",
        "        display_predictions_frequency=display_predictions_frequency\n",
        "      )\n",
        "      dev_metric = dev_stats['dev_denotation_acc']\n",
        "      batch_iterator.set_postfix({'train_loss': (total_loss / len(batch_iterator)), **train_stats, **dev_stats})\n",
        "      if dev_metric > best_metric:\n",
        "        best_epoch = epoch\n",
        "        print(\"Obtained a new best development accuracy of {:.3f}, saving model \"\n",
        "              \"checkpoint to {}...\".format(dev_metric, model_file))\n",
        "        torch.save(model.state_dict(), model_file)\n",
        "        best_metric = dev_metric\n",
        "      stats_by_epoch[epoch] = dev_stats\n",
        "  print(\"Maximal development accuracy of {:.3f}\".format(best_metric))\n",
        "  print(\"Reloading best model checkpoint from {}...\".format(model_file))\n",
        "  model.load_state_dict(torch.load(model_file))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fSEWWcJ-anP"
      },
      "source": [
        "The supervised loss takes each action in the traversal of the correct logical form, collecting the log probabilities of these actions along the way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQNOj1qFRMg5"
      },
      "source": [
        "def supervised_loss(model, instance):\n",
        "  state = model.initialize_model_state(instance)\n",
        "  gold_actions = ParserState.from_logical_form(instance['logical_form']).actions()\n",
        "\n",
        "  loss = torch.tensor(0.0)\n",
        "  for action in gold_actions:\n",
        "    loss += -1.0 * state.action_log_probs[action]\n",
        "    state = state.take_action(action)\n",
        "\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIk7BsGnsAEs"
      },
      "source": [
        "train_dataset = GeoDataset(TRAIN_STATES, word_to_embedding)\n",
        "dev_dataset = GeoDataset(DEV_STATES, word_to_embedding)\n",
        "test_dataset = GeoDataset(TEST_STATES, word_to_embedding)\n",
        "\n",
        "print('{} train instances'.format(len(train_dataset)))\n",
        "print('{} dev instances'.format(len(dev_dataset)))\n",
        "print('{} test instances'.format(len(test_dataset)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNSddI0R0Lg6"
      },
      "source": [
        "supervised_model = Model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_av5k4J4gk1H"
      },
      "source": [
        "We'll now train the model in the cell below. You should obtain a maximal denotation accuracy of at least 78% on the development set. This is a very small dataset and will have some variance. With our reference implementation we obtained 82-83% maximal dev denotation accuracy on average, with a range of 78% to 85% (over 40 trials)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeJGXfuV1StB"
      },
      "source": [
        "train(supervised_model, train_dataset, dev_dataset, 'supervised_model.pt', num_epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ylik2--8tJ_"
      },
      "source": [
        "We'll now view the model predictions on the dev set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7KhfcFD2YeS"
      },
      "source": [
        "evaluate_predictions(\n",
        "  dev_dataset, 'dev', execute,\n",
        "  prediction_function=make_beam_prediction_function(supervised_model),\n",
        "  display_predictions_frequency=5\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSmNww2zIeyl"
      },
      "source": [
        "## Training with Latent Logical Forms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKjm9f9-l5wo"
      },
      "source": [
        "Now, we will relax the assumption that we have logical forms available, and train our model only from question--answer pairs, the databases, and the execution function. This setting, _training from denotations_, requires less supervision but introduces problems of search, delayed reward, and ambiguity, as there are many possible logical forms that can execute to a given denotation. We'll use a classic method, _maximum marginal likelihood_; if you're interested in other methods, see [Misra et al. 2018](https://dipendramisra.com/papers/mchy-emnlp.2018.pdf) and [Guu et al. 2017](https://arxiv.org/abs/1704.07926) for a survey and connections to policy gradient from reinforcement learning.\n",
        "\n",
        "Let $x$ be a question, $z$ be the denotation for the question, and $d$ be a database. Our model gives a distribution $p_\\theta(y \\mid x)$ for logical forms $y$.  Maximum marginal likelihood (approximately) maximizes the log probability of producing the correct denotation $z$, marginalized over logical forms. In practice, we can't sum over all possible logical forms, so we'll approximate using a set of logical forms $\\mathcal{Y}$ returned by beam search:\n",
        "\n",
        "$$J_{MML} = \\log p(z \\mid x, d) \\approx \\log \\sum_{y \\in \\mathcal{Y}} p(z \\mid y, d) p_\\theta(y \\mid x)$$\n",
        "\n",
        "Our execution function (given by the `execute` function we've defined above) is deterministic, and we have $p(z \\mid y, d) = 1$ or $0$ depending on whether $y$ denotes $z$ in the database. We can use this fact to simplify the objective: letting $\\mathcal{C}$ be the subset of _correct_ logical forms $\\mathcal{Y}$, that evaluate to $z$, we have\n",
        "\n",
        "$$J_{MML} = \\log \\sum_{y \\in \\mathcal{C}} p_\\theta(y \\mid x)$$\n",
        "\n",
        "Implement this in the `latent_loss` method below, returning $-J_{MML}$ as the loss, as well as the number of logical forms in $\\mathcal{C}$ (`num_correct`) and the number of logical forms found by search (`num_instances`).\n",
        "\n",
        "__Implementation tip__: for numerical stability, perform calculations in log space using the logical forms' log probabilities and `torch.logsumexp`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EEOPgQ3EmUO"
      },
      "source": [
        "def latent_loss(model, instance, beam_size):\n",
        "  true_denotation = instance['denotation']\n",
        "  world = instance['world']\n",
        "\n",
        "  # contains tuples of model states and total log probabilities returned by beam search: [(model_state: ModelState, log_prob: tensor)]\n",
        "  # some model states may correspond to incomplete logical forms, as when repeated literals are not allowed, it's possible for the model to reach a state where no valid actions are available\n",
        "  # use model_state.parser_state.is_complete to check whether a parser state corresponds to a complete logical form, before attempting to convert it to a logical form\n",
        "  candidates = beam_search(model, instance, beam_size)\n",
        "\n",
        "  num_correct = 0\n",
        "  num_instances = len(candidates)\n",
        "\n",
        "  \"\"\"YOUR CODE HERE\"\"\"\n",
        "\n",
        "  ...\n",
        "\n",
        "  if num_correct == 0:\n",
        "    # no update if no correct examples were found; define a Variable to not break the backprop and update code\n",
        "    return torch.autograd.Variable(torch.tensor(0.0), requires_grad=True), num_correct, num_instances\n",
        "  else:\n",
        "    return loss, num_correct, num_instances\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtniWeF9GA4o"
      },
      "source": [
        "You can sanity check your loss by training and evaluating on a restricted version of the dataset which contains only logical forms with at most two variables and two literals. These examples will be a bit easier, and the constrains on logical forms will allow us to reduce the search space and train faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLJY6gTwDuYn"
      },
      "source": [
        "def is_complex_lf(logical_form):\n",
        "  if len(ParserState.from_logical_form(logical_form).variables_introduced()) > 2:\n",
        "    return True\n",
        "  if ParserState.from_logical_form(logical_form).completed_literal_stack.size > 2:\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "simple_train_dataset = GeoDataset(TRAIN_STATES, word_to_embedding, logical_form_exclude_function=is_complex_lf)\n",
        "simple_dev_dataset = GeoDataset(DEV_STATES, word_to_embedding, logical_form_exclude_function=is_complex_lf)\n",
        "print('{} train instances with maximum 2 variables and 2 literals'.format(len(simple_train_dataset)))\n",
        "print('{} dev instances with maximum 2 variables and 2 literals'.format(len(simple_dev_dataset)))\n",
        "print()\n",
        "for i in range(3):\n",
        "  print_instance(simple_dev_dataset[i])\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHVSx83KD7Iz"
      },
      "source": [
        "simple_latent_model = Model(max_vars=2, max_literals=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VJdX8GmsoTE"
      },
      "source": [
        "The following cell will train your model from denotations alone on these simplified logical forms. \n",
        "\n",
        "With beam size 100, if your `latent_loss` implementation is correct, you should expect to see `train_loss` values roughly between 0.5 and 2 at the end of 10 epochs (note that the magnitude of the loss will change with varying number of candidates).\n",
        "\n",
        "Our implementation obtains _training_ denotation accuracies between 80% and 100%, and dev denotation accuracies of at least 60% by the end of 10 epochs (but note there are only 9 dev examples when filtering the logical forms in this way! so don't be surprised if this score varies widely)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-l3a3OLtEBxc"
      },
      "source": [
        "train(simple_latent_model, simple_train_dataset, simple_dev_dataset, 'simple_latent_model.pt', \n",
        "      num_epochs=10, latent_logical_forms=True, \n",
        "      training_beam_size=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKq3-CwHycwj"
      },
      "source": [
        "evaluate_predictions(\n",
        "  simple_dev_dataset, 'dev', execute,\n",
        "  prediction_function=make_beam_prediction_function(simple_latent_model),\n",
        "  display_predictions_frequency=1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz-6YQ6MLiII"
      },
      "source": [
        "Now, we'll proceed to training on the full dataset with an unrestricted model. The search space is larger here and it may take several epochs before performance starts to increase. While the training loss should consistently decrease, you may see fluctuations in denotation accuracy (both on train and dev) from epoch to epoch. However, you'll likely obtain training denotation accuracies >80% and dev denotation accuracies >70% at some point by epoch 5. Your maximal dev accuracy should be at least 63%, although there is again some variance due to the small dataset size, which is compounded by the variance from training from denotations. Our reference implementation obtained an average maximal dev accuracy of 80%, but ranged between 63% and 90% (over 40 trials). 10 epochs of training should take around 20-30 minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kZlEVV8LtTV"
      },
      "source": [
        "latent_model = Model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoQGZMQuLwk2"
      },
      "source": [
        "train(latent_model, train_dataset, dev_dataset, 'latent_model.pt', \n",
        "      num_epochs=10, latent_logical_forms=True, \n",
        "      training_beam_size=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAhA-fKcL72O"
      },
      "source": [
        "You can run the following cell to see the model predictions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szKxJV_AL63h"
      },
      "source": [
        "evaluate_predictions(\n",
        "  dev_dataset, 'dev', execute,\n",
        "  prediction_function=make_beam_prediction_function(latent_model),\n",
        "  display_predictions_frequency=5\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5r-_Q-aBciM"
      },
      "source": [
        "## Submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBDmSM1-mpnn"
      },
      "source": [
        "def dump_logical_form_actions(dataset, fname):\n",
        "  with open(fname, 'w') as f:\n",
        "    for i in range(len(dataset)):\n",
        "      parser_state = ParserState.from_logical_form(dataset[i]['logical_form'])\n",
        "      f.write('{}\\n'.format(' '.join(parser_state.actions())))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxsS7TmEnE_Z"
      },
      "source": [
        "# to test your ParserState.take_action() method, we'll also save \n",
        "# the actions for the true logical forms\n",
        "dump_logical_form_actions(dev_dataset, './dev_true_lf_actions.txt')\n",
        "dump_logical_form_actions(test_dataset, './test_true_lf_actions.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvANxD_gBdra"
      },
      "source": [
        "# Uncomment this code to restore models from a checkpoint.\n",
        "#\n",
        "# supervised_model = Model(max_vars=4, no_repeated_literals=True)\n",
        "# supervised_model.load_state_dict(torch.load('supervised_model.pt'))\n",
        "# latent_model = Model(max_vars=4, no_repeated_literals=True)\n",
        "# latent_model.load_state_dict(torch.load('latent_model.pt'))\n",
        "\n",
        "pprint.pprint(evaluate_predictions(\n",
        "  dev_dataset, 'supervised_dev', execute,\n",
        "  prediction_function=make_beam_prediction_function(supervised_model, 5),\n",
        "  logical_form_output_file='./supervised_dev_lfs.txt'\n",
        "))\n",
        "pprint.pprint(evaluate_predictions(\n",
        "  test_dataset, 'supervised_test', execute,\n",
        "  prediction_function=make_beam_prediction_function(supervised_model, 5),\n",
        "  logical_form_output_file='./supervised_test_lfs.txt'\n",
        "))\n",
        "pprint.pprint(evaluate_predictions(\n",
        "  dev_dataset, 'latent_dev', execute,\n",
        "  prediction_function=make_beam_prediction_function(latent_model, 5),\n",
        "  logical_form_output_file='./latent_dev_lfs.txt'\n",
        "))\n",
        "pprint.pprint(evaluate_predictions(\n",
        "  test_dataset, 'latent_test', execute,\n",
        "  prediction_function=make_beam_prediction_function(latent_model, 5),\n",
        "  logical_form_output_file='./latent_test_lfs.txt'\n",
        "))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3tuEAhWBj1B"
      },
      "source": [
        "Your submission should consist of your code, the actions to generate true logical forms for the dev and test set (to test your ParserState.take_action() method), as well as the logical forms predicted by your model for both dev and test data in supervised and latent settings. Turn in the following files on Gradescope:\n",
        "\n",
        "- proj_4.ipynb (this file; please rename to match)\n",
        "- dev_true_lf_actions.txt\n",
        "- test_true_lf_actions.txt\n",
        "- supervised_dev_lfs.txt\n",
        "- supervised_test_lfs.txt\n",
        "- latent_dev_lfs.txt\n",
        "- latent_test_lfs.txt\n",
        "\n",
        "Be sure to check the output of the autograder after it runs. It should confirm that no files are missing and that the output files have the correct format."
      ]
    }
  ]
}