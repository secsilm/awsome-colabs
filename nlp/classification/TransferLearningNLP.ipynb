{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TransferLearningNLP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/secsilm/awsome-colabs/blob/master/nlp/classification/TransferLearningNLP.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "JSRl4XtCiffJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning for NLP: Sentiment Analysis on Amazon Reviews\n",
        "In this notebook, we show how transfer learning can be applied to detecting the sentiment of amazon reviews, between positive and negative reviews.\n",
        "\n",
        "This notebook uses the work from [Howard and Ruder, Ulmfit](https://arxiv.org/pdf/1801.06146.pdf).\n",
        "The idea of the paper (and it implementation explained in the [fast.ai deep learning course](http://course.fast.ai/lessons/lesson10.html)) is to learn a language model trained on a very large dataset, e.g. a Wikipedia dump. The intuition is that if a model is able to predict the next word at each word, it means it has learnt something about the structure of the language we are using.\n",
        "\n",
        "[Word2vec](https://arxiv.org/pdf/1310.4546.pdf) and the likes have lead to huge improvements on various NLP tasks. This could be seen as a first step to transfer learning, where the pre-trained word vectors correspond to a transfer of the embedding layer.\n",
        "The ambition of [Ulmfit](https://arxiv.org/pdf/1801.06146.pdf) (and others like [ELMO](https://arxiv.org/pdf/1802.05365.pdf) or the [Transformer language model](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) recently introduced) is to progressively move the NLP field to the state where Computer Vision has risen thanks to the ImageNet challenge. Thanks to the ImageNet chalenge, today it is easy to download a model pre-trained on massive dataset of images, remove the last layer and replace it by a classifier or a regressor depending on the interest. \n",
        "\n",
        "With Ulmfit, the goal is for everyone to be able to use a pre-trained language model and use it a backbone which we can use along with a classifier and a regressor. The game-changing apect of transfer learning is that we are no longer limited by the size of trzining data! With only a fraction of the data size that was necessary before, we can trtain a classifier/regressor and have very good result with few labelled data.\n",
        "\n",
        "Given that labelled text data are difficult to get, in comparison with unlabelled text data which is almost infinite, transfer learning is likely to change radically the field of NLP, and help lead to a maturity state closer to computyer vision.\n",
        "\n",
        "The architecture for the language model used in ULMFit is the [AWD-LSTM language model](https://arxiv.org/pdf/1708.02182.pdf) by Merity.\n",
        "\n",
        "While we are using this language model for this experiment, we keep an eye open to a recently proposed character language model with [Contextual String Embedings](http://alanakbik.github.io/papers/coling2018.pdf) by Akbik."
      ]
    },
    {
      "metadata": {
        "id": "o1gSu3G_iffM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Content of this notebook"
      ]
    },
    {
      "metadata": {
        "id": "PKtvqvWTiffO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This notebook illustrate the power of Ulmfit on a dataset of Amazon reviews available on Kaggle at https://www.kaggle.com/bittlingmayer/amazonreviews/home.\n",
        "We use code from the excellent fastai course and use it for a different dataset. The original code is available at https://github.com/fastai/fastai/tree/master/courses/dl2\n",
        "\n",
        "The data consists of 4M reviews that are either positives or negatives. Training a model with FastText classifier results in a f1 score of 0.916.\n",
        "We show that uing only a fraction of this dataset we are able to reach similar and even better results.\n",
        "\n",
        "We encourage you to try it on your own tasks!\n",
        "Note that if you are interested in Regression instead of classification, you can also do it following this [advice](http://forums.fast.ai/t/regression-using-ulmfit/18063/6)."
      ]
    },
    {
      "metadata": {
        "id": "PrZ5W8m9iffP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The notebook is organized as such:\n",
        "\n",
        "- Tokenize the reviews and create dictionaries\n",
        "- Download a pre-trained model and link the dictionary to the embedding layer of the model\n",
        "- Fine-tune the language model on the amaxon reviews texts\n",
        "\n",
        "We have then the backbone of our algorithm: a pre-trained language model fine-tuned on Amazon reviews\n",
        "\n",
        "- Add a classifier to the language model and train the classifier layer only\n",
        "- Gradually defreeze successive layers to train different layers on the amazon reviews\n",
        "- Run a full classification task for several epochs\n",
        "- Use the model for inference!\n",
        "\n",
        "We end this notebook by looking at the specific effect of training size on the overall performance. This is to test the hypothesis that the ULMFit model does not need much labeled data to perform well."
      ]
    },
    {
      "metadata": {
        "id": "41if5jpWiffR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data"
      ]
    },
    {
      "metadata": {
        "id": "GdeymwL-iffU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Before starting, you should download the data from https://www.kaggle.com/bittlingmayer/amazonreviews, and put the extracted files into an ./Amazon folder somewher you like, and use this path for this notebook.\n",
        "\n",
        "Also, we recommend working on a dedicated environment (e.g. mkvirtualenv fastai). Then clone the fastai github repo https://github.com/fastai/fastai and install requirements."
      ]
    },
    {
      "metadata": {
        "id": "mtOh2d1biffV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from fastai.text import *\n",
        "import html\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import re\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, \\\n",
        "confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from time import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rGArIEgpiffa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path = '/your/path/to/folder/Amazon'\n",
        "train = []\n",
        "with open(os.path.join(path, 'train.ft.txt'), 'r') as file:\n",
        "    for line in file:\n",
        "        train.append(file.readline())\n",
        "        \n",
        "test = []\n",
        "with open(os.path.join(path, 'test.ft.txt'), 'r') as file:\n",
        "    for line in file:\n",
        "        test.append(file.readline())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GXbx2XNKiffe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(f'The train data contains {len(train)} examples')\n",
        "print(f'The test data contains {len(test)} examples')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i61gqnW0iffi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BOS = 'xbos'  # beginning-of-sentence tag\n",
        "FLD = 'xfld'  # data field tag\n",
        "\n",
        "PATH=Path('/your/path/to/folder/Amazon')\n",
        "\n",
        "CLAS_PATH=PATH/'amazon_class'\n",
        "CLAS_PATH.mkdir(exist_ok=True)\n",
        "\n",
        "LM_PATH=PATH/'amazon_lm'\n",
        "LM_PATH.mkdir(exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M7pkl21biffn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Each item is '__label__1/2' and then the review so we split to get texts and labels\n",
        "trn_texts,trn_labels = [text[10:] for text in train], [text[:10] for text in train]\n",
        "trn_labels = [0 if label == '__label__1' else 1 for label in trn_labels]\n",
        "val_texts,val_labels = [text[10:] for text in test], [text[:10] for text in test]\n",
        "val_labels = [0 if label == '__label__1' else 1 for label in val_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0hyC3jYoifft",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Following fast.ai recommendations we put our data in pandas dataframes\n",
        "col_names = ['labels','text']\n",
        "\n",
        "df_trn = pd.DataFrame({'text':trn_texts, 'labels':trn_labels}, columns=col_names)\n",
        "df_val = pd.DataFrame({'text':val_texts, 'labels':val_labels}, columns=col_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XGQvUOGriff1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_trn.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "inkfIZr4ifgH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_trn.to_csv(CLAS_PATH/'train.csv', header=False, index=False)\n",
        "df_val.to_csv(CLAS_PATH/'test.csv', header=False, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EfftlIRSifgM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "CLASSES = ['neg', 'pos']\n",
        "(CLAS_PATH/'classes.txt').open('w').writelines(f'{o}\\n' for o in CLASSES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fhgQ47EUifgP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Language Model"
      ]
    },
    {
      "metadata": {
        "id": "rHP6O_ohifgQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We're going to fine tune the language model so it's ok to take some of the test set in our train data\n",
        "# for the lm fine-tuning\n",
        "trn_texts,val_texts = train_test_split(np.concatenate([trn_texts,val_texts]), test_size=0.1)\n",
        "\n",
        "df_trn = pd.DataFrame({'text':trn_texts, 'labels':[0]*len(trn_texts)}, columns=col_names)\n",
        "df_val = pd.DataFrame({'text':val_texts, 'labels':[0]*len(val_texts)}, columns=col_names)\n",
        "\n",
        "df_trn.to_csv(LM_PATH/'train.csv', header=False, index=False)\n",
        "df_val.to_csv(LM_PATH/'test.csv', header=False, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r0TclUZMifgW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Here we use functions from the fast.ai course to get data\n",
        "\n",
        "chunksize=24000\n",
        "re1 = re.compile(r'  +')\n",
        "\n",
        "def fixup(x):\n",
        "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
        "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
        "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>','u_n').replace(' @.@ ','.').replace(\n",
        "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
        "    return re1.sub(' ', html.unescape(x))\n",
        "\n",
        "def get_texts(df, n_lbls=1):\n",
        "    labels = df.iloc[:,range(n_lbls)].values.astype(np.int64)\n",
        "    texts = f'\\n{BOS} {FLD} 1 ' + df[n_lbls].astype(str)\n",
        "    for i in range(n_lbls+1, len(df.columns)): \n",
        "        texts += f' {FLD} {i-n_lbls} ' + df[i].astype(str)\n",
        "    texts = list(texts.apply(fixup).values)\n",
        "\n",
        "    tok = Tokenizer().proc_all_mp(partition_by_cores(texts))\n",
        "    return tok, list(labels)\n",
        "\n",
        "def get_all(df, n_lbls):\n",
        "    tok, labels = [], []\n",
        "    for i, r in enumerate(df):\n",
        "        print(i)\n",
        "        tok_, labels_ = get_texts(r, n_lbls)\n",
        "        tok += tok_;\n",
        "        labels += labels_\n",
        "    return tok, labels\n",
        "\n",
        "df_trn = pd.read_csv(LM_PATH/'train.csv', header=None, chunksize=chunksize)\n",
        "df_val = pd.read_csv(LM_PATH/'test.csv', header=None, chunksize=chunksize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CqfV1x1fifgZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This cell can take quite some time if your dataset is large\n",
        "# Run it once and comment it for later use\n",
        "tok_trn, trn_labels = get_all(df_trn, 1)\n",
        "tok_val, val_labels = get_all(df_val, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J8rG6V4difge",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Run this cell once and comment everything but the load statements for later use\n",
        "\n",
        "(LM_PATH/'tmp').mkdir(exist_ok=True)\n",
        "np.save(LM_PATH/'tmp'/'tok_trn.npy', tok_trn)\n",
        "np.save(LM_PATH/'tmp'/'tok_val.npy', tok_val)\n",
        "tok_trn = np.load(LM_PATH/'tmp'/'tok_trn.npy')\n",
        "tok_val = np.load(LM_PATH/'tmp'/'tok_val.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HY4a54MMifgi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Check the most common tokens\n",
        "freq = Counter(p for o in tok_trn for p in o)\n",
        "freq.most_common(25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1u30wYGjifgl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Check the least common tokens\n",
        "freq.most_common()[-25:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cr6ds1jBifgn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Build your vocabulary by keeping only the most common tokens that appears frequently enough\n",
        "# and constrain the size of your vocabulary. We follow here the 60k recommendation.\n",
        "max_vocab = 60000\n",
        "min_freq = 2\n",
        "\n",
        "itos = [o for o,c in freq.most_common(max_vocab) if c>min_freq]\n",
        "itos.insert(0, '_pad_')\n",
        "itos.insert(0, '_unk_')\n",
        "\n",
        "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
        "len(itos)\n",
        "\n",
        "trn_lm = np.array([[stoi[o] for o in p] for p in tok_trn])\n",
        "val_lm = np.array([[stoi[o] for o in p] for p in tok_val])\n",
        "\n",
        "np.save(LM_PATH/'tmp'/'trn_ids.npy', trn_lm)\n",
        "np.save(LM_PATH/'tmp'/'val_ids.npy', val_lm)\n",
        "pickle.dump(itos, open(LM_PATH/'tmp'/'itos.pkl', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "staowVK5ifgq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save everything\n",
        "trn_lm = np.load(LM_PATH/'tmp'/'trn_ids.npy')\n",
        "val_lm = np.load(LM_PATH/'tmp'/'val_ids.npy')\n",
        "itos = pickle.load(open(LM_PATH/'tmp'/'itos.pkl', 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H_ra5_LVifgs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vs=len(itos)\n",
        "vs,len(trn_lm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ou1i7jRoifgw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Using pre trained Language Model"
      ]
    },
    {
      "metadata": {
        "id": "owi9Z4eNifgw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Uncomment this cell to download the pre-trained model.\n",
        "# It will be placed into the PATH that you defined earlier.\n",
        "# ! wget -nH -r -np -P {PATH} http://files.fast.ai/models/wt103/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v4i1BJtaifgz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load the weights of the model\n",
        "em_sz,nh,nl = 400,1150,3\n",
        "\n",
        "PRE_PATH = PATH/'models'/'wt103'\n",
        "PRE_LM_PATH = PRE_PATH/'fwd_wt103.h5'\n",
        "\n",
        "wgts = torch.load(PRE_LM_PATH, map_location=lambda storage, loc: storage)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XtMVHf2sifg4",
        "colab_type": "code",
        "colab": {},
        "outputId": "284a4204-e4fd-4c9f-a54b-ac9fe68795ff"
      },
      "cell_type": "code",
      "source": [
        "# Check the word embedding layer and keep a 'mean word' for unknown tokens\n",
        "enc_wgts = to_np(wgts['0.encoder.weight'])\n",
        "row_m = enc_wgts.mean(0)\n",
        "\n",
        "enc_wgts.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(238462, 400)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "zXUt-yA2ifhA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load the vocabulary on which the pre-trained model was trained\n",
        "# Define an embedding matrix with the vocabulary of our dataset\n",
        "itos2 = pickle.load((PRE_PATH/'itos_wt103.pkl').open('rb'))\n",
        "stoi2 = collections.defaultdict(lambda:-1, {v:k for k,v in enumerate(itos2)})\n",
        "\n",
        "new_w = np.zeros((vs, em_sz), dtype=np.float32)\n",
        "for i,w in enumerate(itos):\n",
        "    r = stoi2[w]\n",
        "    new_w[i] = enc_wgts[r] if r>=0 else row_m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zceOkQPmifhE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Use the new embedding matrix for the pre-trained model\n",
        "wgts['0.encoder.weight'] = T(new_w)\n",
        "wgts['0.encoder_with_dropout.embed.weight'] = T(np.copy(new_w))\n",
        "wgts['1.decoder.weight'] = T(np.copy(new_w))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pggOsB4sifhH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define the learner object to do the fine-tuning\n",
        "# Here we will freeze everything except the embedding layer, so that we can have a better \n",
        "# embedding for unknown words than just the mean embedding on which we initialise it.\n",
        "wd=1e-7\n",
        "bptt=70\n",
        "bs=52\n",
        "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
        "\n",
        "trn_dl = LanguageModelLoader(np.concatenate(trn_lm), bs, bptt)\n",
        "val_dl = LanguageModelLoader(np.concatenate(val_lm), bs, bptt)\n",
        "md = LanguageModelData(PATH, 1, vs, trn_dl, val_dl, bs=bs, bptt=bptt)\n",
        "\n",
        "drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.7\n",
        "\n",
        "learner= md.get_model(opt_fn, em_sz, nh, nl, \n",
        "    dropouti=drops[0], dropout=drops[1], wdrop=drops[2], dropoute=drops[3], dropouth=drops[4])\n",
        "\n",
        "learner.metrics = [accuracy]\n",
        "learner.freeze_to(-1)\n",
        "\n",
        "learner.model.load_state_dict(wgts)\n",
        "\n",
        "lr=1e-3\n",
        "lrs = lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TS0ZjK_JifhL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Run one epoch of fine-tuning \n",
        "learner.fit(lrs/2, 1, wds=wd, use_clr=(32,2), cycle_len=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9_t_ZaYIifhO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model and unfreeze everything to later fine-tune the whole model\n",
        "learner.save('lm_last_ft')\n",
        "learner.load('lm_last_ft')\n",
        "learner.unfreeze()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nZ5re6TjifhQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learner.lr_find(start_lr=lrs/10, end_lr=lrs*10, linear=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bLjk62OSifhU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learner.sched.plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y7iWrHMYifhZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Run this if you want to highly tune the LM to the Amazon data, with 15 epochs\n",
        "# use_clr controls the shape of the cyclical (triangular) learning rate\n",
        "learner.fit(lrs, 1, wds=wd, use_clr=(20,10), cycle_len=15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xUV9iHc9ifhb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save the Backbone for further classification!!\n",
        "learner.save('lm1')\n",
        "learner.save_encoder('lm1_enc')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4V3XhAukifhc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learner.sched.plot_loss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zGJ9fV_Difhf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Going back to classification!"
      ]
    },
    {
      "metadata": {
        "id": "svJCQQ-sifhg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now that we spent some time fine-tuning the language model on our Amazon data, let's see if we can classify easily these reviews.\n",
        "As before, some cells should be run once, and then use data loaders for later use."
      ]
    },
    {
      "metadata": {
        "id": "mPkDJ9sLifhg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_trn = pd.read_csv(CLAS_PATH/'train.csv', header=None, chunksize=chunksize)\n",
        "df_val = pd.read_csv(CLAS_PATH/'test.csv', header=None, chunksize=chunksize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OELAfoM_ifhi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tok_trn, trn_labels = get_all(df_trn, 1)\n",
        "tok_val, val_labels = get_all(df_val, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "STFczwD6ifhk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "(CLAS_PATH/'tmp').mkdir(exist_ok=True)\n",
        "\n",
        "np.save(CLAS_PATH/'tmp'/'tok_trn.npy', tok_trn)\n",
        "np.save(CLAS_PATH/'tmp'/'tok_val.npy', tok_val)\n",
        "\n",
        "np.save(CLAS_PATH/'tmp'/'trn_labels.npy', trn_labels)\n",
        "np.save(CLAS_PATH/'tmp'/'val_labels.npy', val_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rVGvPRc9ifhm",
        "colab_type": "code",
        "colab": {},
        "outputId": "d164f8ef-79da-4953-8079-8798d52125e4"
      },
      "cell_type": "code",
      "source": [
        "tok_trn = np.load(CLAS_PATH/'tmp'/'tok_trn.npy')\n",
        "tok_val = np.load(CLAS_PATH/'tmp'/'tok_val.npy')\n",
        "itos = pickle.load((LM_PATH/'tmp'/'itos.pkl').open('rb'))\n",
        "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
        "len(itos)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60002"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "V2Ws1grbifhu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trn_clas = np.array([[stoi[o] for o in p] for p in tok_trn])\n",
        "val_clas = np.array([[stoi[o] for o in p] for p in tok_val])\n",
        "\n",
        "np.save(CLAS_PATH/'tmp'/'trn_ids.npy', trn_clas)\n",
        "np.save(CLAS_PATH/'tmp'/'val_ids.npy', val_clas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6bXvrHJWifhw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Classifier\n",
        "In this part, we adopt an unusual train/test hierarchy. While it's common to train on a big dataset and thewn test on a small one, here we wanrt to test the hypothesis that the model can learn with few training data. Hence we take less data for training than for testing."
      ]
    },
    {
      "metadata": {
        "id": "2_O9Fv3Gifhw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We select here the 'size' first reviews of our dataset\n",
        "# The paper claims that it's possible to achieve very good results with few labeled examples\n",
        "# So let's try with 100 examples for training, and 5000 examples for validation.\n",
        "# We encourage you to try different values to see the effect of data size on performance.\n",
        "trn_size = 100\n",
        "val_size = 5000\n",
        "trn_clas = np.load(CLAS_PATH/'tmp'/'trn_ids.npy')\n",
        "val_clas = np.load(CLAS_PATH/'tmp'/'val_ids.npy')\n",
        "\n",
        "trn_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'trn_labels.npy'))\n",
        "val_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'val_labels.npy'))\n",
        "\n",
        "train = random.sample(list(zip(trn_clas, trn_labels)), trn_size)\n",
        "trn_clas = np.array([item[0] for item in train])\n",
        "trn_labels = np.array([item[1] for item in train])\n",
        "del train\n",
        "\n",
        "validation = random.sample(list(zip(val_clas, val_labels)), val_size)\n",
        "val_clas = np.array([item[0] for item in validation])\n",
        "val_labels = np.array([item[1] for item in validation])\n",
        "del validation\n",
        "\n",
        "\n",
        "bptt,em_sz,nh,nl = 70,400,1150,3\n",
        "vs = len(itos)\n",
        "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
        "bs = 48\n",
        "\n",
        "min_lbl = trn_labels.min()\n",
        "trn_labels -= min_lbl\n",
        "val_labels -= min_lbl\n",
        "c=int(trn_labels.max())+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L5lEZKwwifhy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Ccheck that the validation dataset is well balanced so acccuracy is a good metric\n",
        "# We'll also check other metrics usual for binary classification (precision, recall, f1 score)\n",
        "len(trn_labels[trn_labels == 1]) / len(trn_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-8AckmXwifhz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trn_ds = TextDataset(trn_clas, trn_labels)\n",
        "val_ds = TextDataset(val_clas, val_labels)\n",
        "trn_samp = SortishSampler(trn_clas, key=lambda x: len(trn_clas[x]), bs=bs//2)\n",
        "val_samp = SortSampler(val_clas, key=lambda x: len(val_clas[x]))\n",
        "trn_dl = DataLoader(trn_ds, bs//2, transpose=True, num_workers=1, pad_idx=1, sampler=trn_samp)\n",
        "val_dl = DataLoader(val_ds, bs, transpose=True, num_workers=1, pad_idx=1, sampler=val_samp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N_nF_hJ-ifh3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We define the model, here it a classifier on top of an RNN language model\n",
        "# We load the language model encoder that we fine tuned before\n",
        "# We freeze everything but the last layer, so that we can train the classification layer only.\n",
        "#load the saved weights from before, and freeze everything until the last layer\n",
        "\n",
        "md = ModelData(PATH, trn_dl, val_dl)\n",
        "dps = np.array([0.4, 0.5, 0.05, 0.3, 0.1])\n",
        "\n",
        "m = get_rnn_classifier(bptt, 20*70, c, vs, emb_sz=em_sz, n_hid=nh, n_layers=nl, pad_token=1,\n",
        "          layers=[em_sz*3, 50, c], drops=[dps[4], 0.1],\n",
        "          dropouti=dps[0], wdrop=dps[1], dropoute=dps[2], dropouth=dps[3])\n",
        "\n",
        "opt_fn = partial(optim.Adam, betas=(0.7, 0.99))\n",
        "\n",
        "learn = RNN_Learner(md, TextModel(to_gpu(m)), opt_fn=opt_fn)\n",
        "learn.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
        "learn.clip=25.\n",
        "learn.metrics = [accuracy]\n",
        "\n",
        "lr=3e-3\n",
        "lrm = 2.6\n",
        "lrs = np.array([lr/(lrm**4), lr/(lrm**3), lr/(lrm**2), lr/lrm, lr])\n",
        "\n",
        "lrs=np.array([1e-4,1e-4,1e-4,1e-3,1e-2])\n",
        "\n",
        "wd = 1e-7\n",
        "wd = 0\n",
        "learn.load_encoder('lm1_enc')\n",
        "\n",
        "learn.freeze_to(-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mNgGl8G-ifh6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.lr_find(lrs/1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7S0U-X5nifh8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.sched.plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yp9H-zyhifh_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Run one epoch on the classification layer\n",
        "learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MDY2PRaeifiD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "learn.save('clas_0')\n",
        "learn.load('clas_0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ilkyQyhNifiE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Gradually unfreeze another layer to train a bit more parameters than just the classifier layer\n",
        "learn.freeze_to(-2)\n",
        "learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FRDqBfnWifiG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "learn.save('clas_1')\n",
        "learn.load('clas_1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4kbWM7FUifiK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Unfreeze everything and train for a few epochs on the whole set of parameters of the model\n",
        "learn.unfreeze()\n",
        "learn.fit(lrs, 1, wds=wd, cycle_len=14, use_clr=(32,10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qUdQCdFCifiM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.sched.plot_loss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XWWxZVJyifiO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "learn.save('clas_2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TenH511JifiQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Inference\n",
        "Nonw, let's play with the model we've just learned!"
      ]
    },
    {
      "metadata": {
        "id": "fwh4-1aeifiQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m = get_rnn_classifer(bptt, 20*70, c, vs, emb_sz=em_sz, n_hid=nh, n_layers=nl, pad_token=1,\n",
        "          layers=[em_sz*3, 50, c], drops=[dps[4], 0.1],\n",
        "          dropouti=dps[0], wdrop=dps[1], dropoute=dps[2], dropouth=dps[3])\n",
        "opt_fn = partial(optim.Adam, betas=(0.7, 0.99))\n",
        "learn = RNN_Learner(md, TextModel(to_gpu(m)), opt_fn=opt_fn)\n",
        "learn.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
        "learn.clip=25.\n",
        "learn.metrics = [accuracy]\n",
        "\n",
        "lr=3e-3\n",
        "lrm = 2.6\n",
        "lrs = np.array([lr/(lrm**4), lr/(lrm**3), lr/(lrm**2), lr/lrm, lr])\n",
        "wd = 1e-7\n",
        "wd = 0\n",
        "learn.load_encoder('lm1_enc')\n",
        "learn.load('clas_2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VXjKsbGMifiS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_sentiment(input_str: str):\n",
        "\n",
        "    # predictions are done on arrays of input.\n",
        "    # We only have a single input, so turn it into a 1x1 array\n",
        "    texts = [input_str]\n",
        "\n",
        "    # tokenize using the fastai wrapper around spacy\n",
        "    tok = [t.split() for t in texts]\n",
        "    # tok = Tokenizer().proc_all_mp(partition_by_cores(texts))\n",
        "\n",
        "    # turn into integers for each word\n",
        "    encoded = [stoi[p] for p in tok[0]]\n",
        "\n",
        "    idx = np.array(encoded)[None]\n",
        "    idx = np.transpose(idx)\n",
        "    tensorIdx = VV(idx)\n",
        "    m.eval()\n",
        "    m.reset()\n",
        "    p = m.forward(tensorIdx)\n",
        "    return np.argmax(p[0][0].data.cpu().numpy())\n",
        "\n",
        "def prediction(texts):\n",
        "    \"\"\"Do the prediction on a list of texts\n",
        "    \"\"\"\n",
        "    y = []\n",
        "    \n",
        "    for i, text in enumerate(texts):\n",
        "        if i % 1000 == 0:\n",
        "            print(i)\n",
        "        encoded = text\n",
        "        idx = np.array(encoded)[None]\n",
        "        idx = np.transpose(idx)\n",
        "        tensorIdx = VV(idx)\n",
        "        m.eval()\n",
        "        m.reset()\n",
        "        p = m.forward(tensorIdx)\n",
        "        y.append(np.argmax(p[0][0].data.cpu().numpy()))\n",
        "    return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S9grziDhifiV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentence = \"I like Feedly\"\n",
        "start = time()\n",
        "print(get_sentiment(sentence))\n",
        "print(time() - start)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kBeU3lsZifiW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y = prediction(list(val_clas))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BAhX3D1JifiY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Show relevant metrics for binary classification\n",
        "# We encourage you to try training the classifier with different data size and its effect on performance\n",
        "print(f'Accuracy --> {accuracy_score(y, val_labels)}')\n",
        "print(f'Precision --> {precision_score(y, val_labels)}')\n",
        "print(f'F1 score --> {f1_score(y, val_labels)}')\n",
        "print(f'Recall score --> {recall_score(y, val_labels)}')\n",
        "print(confusion_matrix(y, val_labels))\n",
        "print(classification_report(y, val_labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a2G65x8oifia",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# What training size do we need?\n",
        "The language model has already learnt a lot about the syntax. It is very knowledgeable about the context in which words appear in sentences. However, the language model does not contain any notion of [meaning](https://en.wikipedia.org/wiki/Meaning_%28linguistics%29). This problem is well summarised in [Emily Bender's tweet](https://twitter.com/emilymbender/status/1024042044035985408) during a very interesting twiter thread that occur in July around meaning in NLP. A cool summary of this thread can be found in the [Hugging Face](https://medium.com/huggingface/learning-meaning-in-natural-language-processing-the-semantics-mega-thread-9c0332dfe28e) blogpost. Hence the meaning in language is very likely to be learned through supervision, with the help of ground-truth examples.\n",
        "\n",
        "However, when we perform some NLP tasks, sentiment analysis in our example, both syntax and meaning are important!\n",
        "The idea is that you can save a lot of time by being taught with a lot of blind synatx first, and then learning meaning. Think of when you start learning a complete new field. Well, it is far easier to learn it in your mother tongue than in another language you master less. \n",
        "\n",
        "The big practical gain here is that once you \"know\" a language, you need less supervised examples to learn a new thing! In our example, it means we need less labeled reviews for us to learn a relevant classifier.\n",
        "\n",
        "Let's verify this hypothesis by training a classifier with several training size and see how this size affects the performance!"
      ]
    },
    {
      "metadata": {
        "id": "jTeGzAKzific",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trn_clas = np.load(CLAS_PATH/'tmp'/'trn_ids.npy')\n",
        "val_clas = np.load(CLAS_PATH/'tmp'/'val_ids.npy')\n",
        "\n",
        "trn_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'trn_labels.npy'))\n",
        "val_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'val_labels.npy'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "46col76aifif",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def experiment(trn_size, val_size):\n",
        "\n",
        "    train = random.sample(list(zip(trn_clas, trn_labels)), trn_size)\n",
        "    aux_trn_clas = np.array([item[0] for item in train])\n",
        "    aux_trn_labels = np.array([item[1] for item in train])\n",
        "    del train\n",
        "\n",
        "    validation = random.sample(list(zip(val_clas, val_labels)), val_size)\n",
        "    aux_val_clas = np.array([item[0] for item in validation])\n",
        "    aux_val_labels = np.array([item[1] for item in validation])\n",
        "    del validation\n",
        "\n",
        "\n",
        "    bptt,em_sz,nh,nl = 70,400,1150,3\n",
        "    vs = len(itos)\n",
        "    opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
        "    bs = 48\n",
        "\n",
        "    min_lbl = aux_trn_labels.min()\n",
        "    aux_trn_labels -= min_lbl\n",
        "    aux_val_labels -= min_lbl\n",
        "    c=int(aux_trn_labels.max())+1\n",
        "\n",
        "    # Load data in relevant structures\n",
        "    trn_ds = TextDataset(aux_trn_clas, aux_trn_labels)\n",
        "    val_ds = TextDataset(aux_val_clas, aux_val_labels)\n",
        "    trn_samp = SortishSampler(aux_trn_clas, key=lambda x: len(aux_trn_clas[x]), bs=bs//2)\n",
        "    val_samp = SortSampler(aux_val_clas, key=lambda x: len(aux_val_clas[x]))\n",
        "    trn_dl = DataLoader(trn_ds, bs//2, transpose=True, num_workers=1, pad_idx=1, sampler=trn_samp)\n",
        "    val_dl = DataLoader(val_ds, bs, transpose=True, num_workers=1, pad_idx=1, sampler=val_samp)\n",
        "\n",
        "    # Define the model and load the backbone lamguage model\n",
        "    md = ModelData(PATH, trn_dl, val_dl)\n",
        "    dps = np.array([0.4, 0.5, 0.05, 0.3, 0.1])\n",
        "\n",
        "    m = get_rnn_classifier(bptt, 20*70, c, vs, emb_sz=em_sz, n_hid=nh, n_layers=nl, pad_token=1,\n",
        "              layers=[em_sz*3, 50, c], drops=[dps[4], 0.1],\n",
        "              dropouti=dps[0], wdrop=dps[1], dropoute=dps[2], dropouth=dps[3])\n",
        "\n",
        "    opt_fn = partial(optim.Adam, betas=(0.7, 0.99))\n",
        "\n",
        "    learn = RNN_Learner(md, TextModel(to_gpu(m)), opt_fn=opt_fn)\n",
        "    learn.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
        "    learn.clip=25.\n",
        "    learn.metrics = [accuracy]\n",
        "\n",
        "    lr=3e-3\n",
        "    lrm = 2.6\n",
        "    lrs = np.array([lr/(lrm**4), lr/(lrm**3), lr/(lrm**2), lr/lrm, lr])\n",
        "\n",
        "    lrs=np.array([1e-4,1e-4,1e-4,1e-3,1e-2])\n",
        "\n",
        "    wd = 1e-7\n",
        "    wd = 0\n",
        "    learn.load_encoder('lm1_enc')\n",
        "\n",
        "    learn.freeze_to(-1)\n",
        "\n",
        "    # Find th learning rate\n",
        "    learn.lr_find(lrs/1000)\n",
        "\n",
        "    # Run one epoch on the classification layer\n",
        "    learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))\n",
        "\n",
        "    # Save the trained model\n",
        "    learn.save(f'{trn_size}clas_0')\n",
        "    learn.load(f'{trn_size}clas_0')\n",
        "\n",
        "    # Gradually unfreeze another layer to train a bit more parameters than just the classifier layer\n",
        "    learn.freeze_to(-2)\n",
        "    learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))\n",
        "\n",
        "    # Save the trained model\n",
        "    learn.save(f'{trn_size}clas_1')\n",
        "    learn.load(f'{trn_size}clas_1')\n",
        "\n",
        "    # Unfreeze everything and train for a few epochs on the whole set of parameters of the model\n",
        "    learn.unfreeze()\n",
        "    learn.fit(lrs, 1, wds=wd, cycle_len=14, use_clr=(32,10))\n",
        "\n",
        "    # Save the model\n",
        "    learn.sched.plot_loss()\n",
        "    learn.save(f'{trn_size}clas_2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rF2ypGx9ifij",
        "colab_type": "code",
        "colab": {},
        "outputId": "d234fd93-2c87-4586-9b13-a55e91e8b88a"
      },
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "val_size = 100000\n",
        "for trn_size in [50, 100, 500, 1000, 5000, 10000, 20000, 50000]:\n",
        "    print('#'*50)\n",
        "    print(f'Experiment with training size {trn_size}')\n",
        "    start = time()\n",
        "    experiment(trn_size, val_size)\n",
        "    t = time() - start\n",
        "    print(f'Time cost: {t}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "##################################################\n",
            "Experiment with training size 50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "527d42dc659a450da6729240a5031473",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch      trn_loss   val_loss   accuracy                \n",
            "    0      0.739306   0.713452   0.515713  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee8c2bcd3caa40509641c4735fa0af4f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch      trn_loss   val_loss   accuracy                \n",
            "    0      0.780253   0.682528   0.60368   \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d79ca2874ce4160a029781cd75042f8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch      trn_loss   val_loss   accuracy                \n",
            "    0      0.480616   0.665205   0.64434   \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff07a3dc2f0341b5953d7589bc45d242",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=14), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "    1      0.60659    0.642443   0.60112                 \n",
            "    2      0.61519    0.619721   0.69608                 \n",
            "    3      0.642923   0.626678   0.61732                 \n",
            "    4      0.652647   0.660426   0.51752                 \n",
            "    5      0.602682   0.620081   0.5915                  \n",
            "    6      0.594284   0.584023   0.66818                 \n",
            "    7      0.58685    0.559354   0.73106                 \n",
            "    8      0.55382    0.540782   0.77018                 \n",
            "    9      0.52772    0.527295   0.79862                 \n",
            "    10     0.518118   0.487917   0.83798                 \n",
            "    11     0.518521   0.461052   0.84442                 \n",
            "    12     0.53044    0.453327   0.84558                 \n",
            "    13     0.510322   0.468408   0.83854                 \n",
            "Time cost: 3158.540988445282\n",
            "##################################################\n",
            "Experiment with training size 100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad60ac5683bd408992d4c89c436c2daf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch      trn_loss   val_loss   accuracy                \n",
            "    0      0.598895   78.284828  0.482651  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5124bc474f48464487b3f3e1b58ae6bc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch      trn_loss   val_loss   accuracy                \n",
            "    0      0.62033    0.664568   0.71318   \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e5fe7d0a84f64499b389d7c14a44854f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch      trn_loss   val_loss   accuracy                \n",
            "    0      0.602062   0.617134   0.74574   \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aaeca1c3aa2b4a9cbeabe672487990fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=14), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch      trn_loss   val_loss   accuracy                \n",
            "    0      0.509279   0.616894   0.58494   \n",
            "    1      0.528293   0.574365   0.69924                 \n",
            "    2      0.496826   0.544474   0.75798                 \n",
            "    3      0.478803   0.559163   0.6684                  \n",
            "    4      0.442439   0.568413   0.64396                 \n",
            "    5      0.45688    0.435576   0.82176                 \n",
            "    6      0.438374   0.401803   0.87232                 \n",
            "    7      0.435346   0.382793   0.86982                 \n",
            "    8      0.430963   0.38687    0.86138                 \n",
            "    9      0.421749   0.363613   0.86442                 \n",
            "    10     0.404818   0.347554   0.87324                 \n",
            "    11     0.402366   0.34878    0.8688                  \n",
            "    12     0.420744   0.341431   0.86758                 \n",
            "    13     0.405834   0.34154    0.86362                 \n",
            "Time cost: 3164.5589134693146\n",
            "##################################################\n",
            "Experiment with training size 500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb902a8b103341478714ea65b737d627",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8503deab63514cf4ad216d70c1c6d27e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch      trn_loss   val_loss   accuracy                  \n",
            "    0      0.531424   0.558967   0.85856   \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "09bb2a30cd8c41f89c4f419b504d5a87",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch      trn_loss   val_loss   accuracy                  \n",
            "    0      0.427045   0.402448   0.88602   \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a15c2e251094cbf8b86b7eae495b92d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=14), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch      trn_loss   val_loss   accuracy                  \n",
            "    0      0.43276    0.325113   0.88386   \n",
            "    1      0.439859   0.350954   0.85564                   \n",
            "    2      0.420882   0.301699   0.88072                   \n",
            "    3      0.408916   0.243965   0.91232                   \n",
            "    4      0.385137   0.265443   0.8924                    \n",
            "    5      0.374238   0.249731   0.89888                   \n",
            "    6      0.397431   0.265853   0.90392                   \n",
            "    7      0.388508   0.256725   0.90612                   \n",
            "    8      0.405042   0.269658   0.90676                   \n",
            "    9      0.3749     0.278558   0.89718                   \n",
            "    10     0.378312   0.280107   0.89688                   \n",
            "    11     0.368829   0.269968   0.90122                   \n",
            "    12     0.412016   0.274945   0.90104                   \n",
            "    13     0.399776   0.281551   0.89786                   \n",
            "Time cost: 3095.5910897254944\n",
            "##################################################\n",
            "Experiment with training size 1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "612a9871f9a84d9cbaf1d58e8471a242",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0caf2cfa4414c0080a551270ba43207",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch      trn_loss   val_loss   accuracy                  \n",
            "    0      0.538816   0.369876   0.90136   \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b6f8e0bf835441088904e22ce529794b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch      trn_loss   val_loss   accuracy                  \n",
            "    0      0.453464   0.315374   0.88258   \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a52a6708906433c9b1bfde614fcf859",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=14), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch      trn_loss   val_loss   accuracy                  \n",
            "    0      0.404357   0.259631   0.90256   \n",
            "    1      0.419865   0.254745   0.89808                   \n",
            "    2      0.445964   0.268253   0.89904                   \n",
            "    3      0.427022   0.229095   0.91462                   \n",
            "    4      0.414167   0.228874   0.91148                   \n",
            "    5      0.407483   0.219707   0.91912                   \n",
            "    6      0.381847   0.216046   0.9203                    \n",
            "    7      0.365503   0.219289   0.91962                   \n",
            "    8      0.358103   0.213313   0.92152                   \n",
            "    9      0.328652   0.219443   0.91694                   \n",
            "    10     0.360773   0.225698   0.9129                    \n",
            "    11     0.325618   0.216891   0.91786                   \n",
            "    12     0.358954   0.213793   0.91994                   \n",
            "    13     0.324676   0.217357   0.91804                   \n",
            "Time cost: 3222.9498105049133\n",
            "##################################################\n",
            "Experiment with training size 5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d427fdd87c0f4503a4071c1af8ab976f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            " 80%|████████  | 168/209 [00:33<00:08,  5.06it/s, loss=2.11] "
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3eba1d18c6ce49fab0ce55092d8289aa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch      trn_loss   val_loss   accuracy                    \n",
            "    0      0.476658   0.251208   0.91892   \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ab5437ff9b5243cfb8167a58dbc9392c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch      trn_loss   val_loss   accuracy                    \n",
            "    0      0.433952   0.231621   0.92414   \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b5b7a98c33b14bab9419ff6a0017aca9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=14), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch      trn_loss   val_loss   accuracy                    \n",
            "    0      0.441624   0.26157    0.91548   \n",
            "    1      0.39728    0.216438   0.92384                     \n",
            "    2      0.409002   0.224356   0.92368                     \n",
            "    3      0.422964   0.215129   0.92208                     \n",
            "    5      0.323477   0.190459   0.92822                     \n",
            "    6      0.359594   0.204132   0.9299                      \n",
            "    7      0.364609   0.197063   0.92962                     \n",
            "    8      0.335434   0.195078   0.93054                     \n",
            "    9      0.344869   0.193901   0.93174                     \n",
            "    10     0.355132   0.204457   0.92736                     \n",
            "    11     0.361977   0.196434   0.92986                     \n",
            "    12     0.335396   0.200645   0.92896                     \n",
            "    13     0.327323   0.20609    0.92624                     \n",
            "Time cost: 4408.779232263565\n",
            "##################################################\n",
            "Experiment with training size 10000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f45368cce77460e99e57503a0aaa86a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            " 77%|███████▋  | 323/417 [00:54<00:15,  5.95it/s, loss=1.4]  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b682accf3d4426db72b169099aebc1b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch      trn_loss   val_loss   accuracy                    \n",
            "    0      0.442663   0.237719   0.91894   \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "874d6b51c4f741c09dac9aa917dd1b44",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch      trn_loss   val_loss   accuracy                    \n",
            "    0      0.431919   0.23883    0.92334   \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d92179a697634fa8849aca16e5354f0a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=14), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch      trn_loss   val_loss   accuracy                    \n",
            "    0      0.423774   0.199739   0.92554   \n",
            "    1      0.400266   0.206542   0.92344                     \n",
            "    2      0.327765   0.191927   0.93002                     \n",
            "    3      0.355688   0.193465   0.92908                     \n",
            "    4      0.336286   0.182849   0.93128                     \n",
            "    5      0.324608   0.18046    0.93278                     \n",
            "    6      0.314902   0.183413   0.93328                     \n",
            "    7      0.328284   0.178485   0.93288                     \n",
            "    8      0.337061   0.180216   0.93436                     \n",
            "    9      0.308937   0.179975   0.9341                      \n",
            "    10     0.290357   0.178364   0.93366                     \n",
            "    11     0.301147   0.175089   0.93584                     \n",
            "    12     0.267383   0.176672   0.934                       \n",
            "    13     0.305133   0.17432    0.93538                     \n",
            "Time cost: 5908.472403526306\n",
            "##################################################\n",
            "Experiment with training size 20000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f3ca9f298094380949988fdef48a975",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            " 75%|███████▍  | 623/834 [01:46<00:36,  5.84it/s, loss=1.45] "
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "98d4e901bff745158fb949b6967242b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch      trn_loss   val_loss   accuracy                    \n",
            "    0      0.425248   0.229867   0.91804   \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "10b105d862fe4db9b180c4122105e5d6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch      trn_loss   val_loss   accuracy                    \n",
            "    0      0.410012   0.210839   0.92766   \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c13f18e98fb244a4b86697eec5088914",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=14), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch      trn_loss   val_loss   accuracy                    \n",
            "    0      0.418405   0.202191   0.92848   \n",
            "    1      0.385172   0.21752    0.92934                     \n",
            "    2      0.341867   0.1879     0.93032                     \n",
            "    3      0.343511   0.176737   0.93358                     \n",
            "    4      0.299173   0.169992   0.9357                      \n",
            " 58%|█████▊    | 480/834 [03:46<02:46,  2.12it/s, loss=0.315]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "    9      0.313465   0.162371   0.93966                     \n",
            "    10     0.2692     0.162227   0.93946                     \n",
            "    11     0.272758   0.159716   0.94032                     \n",
            " 80%|███████▉  | 666/834 [04:43<01:11,  2.35it/s, loss=0.261]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch      trn_loss   val_loss   accuracy                      \n",
            "    0      0.441473   0.254497   0.9168    \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e715be6b7434946857e005128042985",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            " 98%|█████████▊| 2044/2084 [07:08<00:08,  4.77it/s, loss=0.414]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "    1      0.309567   0.170769   0.9367                        \n",
            " 80%|███████▉  | 1664/2084 [11:45<02:57,  2.36it/s, loss=0.249]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "    4      0.257701   0.153826   0.9416                        \n",
            " 80%|███████▉  | 1665/2084 [13:25<03:22,  2.07it/s, loss=0.239]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "    6      0.24764    0.148807   0.94436                       \n",
            "    7      0.239934   0.146907   0.9456                        \n",
            "    8      0.224837   0.156241   0.94496                       \n",
            "  9%|▉         | 189/2084 [01:18<13:09,  2.40it/s, loss=0.212]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "    10     0.212315   0.145792   0.94616                       \n",
            "    11     0.221374   0.14564    0.9458                        \n",
            "  8%|▊         | 166/2084 [01:09<13:18,  2.40it/s, loss=0.186]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cGgz9HaPifin",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Some notebook issues here, you might want to run this cell from a python script..."
      ]
    },
    {
      "metadata": {
        "id": "zrRafu35ifio",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Conclusions\n",
        "Lety's see the evollution of the accuracy when we increas the size of the train data.\n",
        "For each training size, we report the best accuracy among the different epochs."
      ]
    },
    {
      "metadata": {
        "id": "BP6Sf7taifio",
        "colab_type": "code",
        "colab": {},
        "outputId": "d220f391-a15a-4c61-b8cc-d8f2140ea3e2"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "best_acc = [0.84558, 0.87324, 0.91232, 0.9203, 0.93174, 0.93584, 0.94032, 0.94616]\n",
        "sizes = [50, 100, 500, 1000, 5000, 10000, 20000, 50000]\n",
        "plt.plot(sizes, best_acc)\n",
        "plt.title('Evolution of performance when increasing the training size')\n",
        "plt.xlabel('Training size')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(sizes, best_acc)\n",
        "plt.title('Evolution of performance when increasing the training size, Zoom on the [0-10000] size zone')\n",
        "plt.xlabel('Training size')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlim([0, 10000])\n",
        "plt.show()\n",
        "\n",
        "plt.plot(np.log(sizes)/np.log(10), best_acc)\n",
        "plt.title('Evolution of performance when increasing the training size, with log scale for size')\n",
        "plt.xlabel('Training size (log)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "uxdD7PDpifit",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- The first observation is, even with 50 samples only, we get a pretty great accuracy of 0.85!\n",
        "- Then we see that the learning progress is very consequent when going from a size of 50 to 1000 samples\n",
        "- The ULMFit beats the reported score from FastText (~0.92) when using 1000 samples only! Note that the reported score from FastText is from a training using the whole training data (3.6M samples)\n",
        "- The accuracy continues to rise when we increase the training size, but with a lower speed. Here the trade-off comes, where you have to decide whether the extra 0.1% in accuracy is worth paying for more labeled data!\n",
        "- From the log-scale graph we might expect even greater results when raining the training size. We have 4.6M training reviews so we could get orders of magnitude more so we could expect reaching 0.95 accuracy or more with the full dataset."
      ]
    }
  ]
}